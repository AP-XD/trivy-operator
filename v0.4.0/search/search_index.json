{"config":{"lang":["en"],"separator":"[\\s\\-]+"},"docs":[{"title":"Home","text":"<p>Trivy has a native Kubernetes Operator which continuously scans your Kubernetes cluster for security issues, and generates security reports as Kubernetes Custom Resources. It does it by watching Kubernetes for state changes and automatically triggering scans in response to changes, for example initiating a vulnerability scan when a new Pod is created.</p>  <p>Trivy Operator is based on existing Aqua OSS project - [Starboard], and shares some of the design, principles and code with it. Existing content that relates to Starboard Operator might also be relevant for Trivy Operator. To learn more about the transition from Starboard to Trivy, see the announcement discussion.</p>    Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects.","location":""},{"title":"Frequently Asked Questions","text":"","location":"faq/"},{"title":"Why do you duplicate instances of VulnerabilityReports for the same image digest?","text":"<p>Docker image reference is not a first class citizen in Kubernetes. It's a property of the container definition. Trivy-operator relies on label selectors to associate VulnerabilityReports with corresponding Kubernetes workloads, not particular image references. For example, we can get all reports for the wordpress Deployment with the following command:</p> <pre><code>kubectl get vulnerabilityreports \\\n  -l trivy-operator.resource.kind=Deployment \\\n  -l trivy-operator.resource.name=wordpress\n</code></pre> <p>Beyond that, for each instance of the VulnerabilityReports we set the owner reference pointing to the corresponding pods controller. By doing that we can manage orphaned VulnerabilityReports and leverage Kubernetes garbage collection. For example, if the <code>wordpress</code> Deployment is deleted, all related VulnerabilityReports are automatically garbage collected.</p>","location":"faq/#why-do-you-duplicate-instances-of-vulnerabilityreports-for-the-same-image-digest"},{"title":"Why do you create an instance of the VulnerabilityReport for each container?","text":"<p>The idea is to partition VulnerabilityReports generated for a particular Kubernetes workload by containers is to mitigate the risk of exceeding the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p>","location":"faq/#why-do-you-create-an-instance-of-the-vulnerabilityreport-for-each-container"},{"title":"Settings","text":"<p>Trivy Operator read configuration settings from ConfigMaps, as well as Secrets that holds confidential settings (such as a GitHub token). Trivy-Operator plugins read configuration and secret data from ConfigMaps and Secrets named after the plugin. For example, Trivy configuration is stored in the ConfigMap and Secret named <code>trivy-operator-trivy-config</code>.</p> <p>You can change the default settings with <code>kubectl patch</code> or <code>kubectl edit</code> commands. For example, by default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). However, you can display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>trivy-operator-trivy-config</code> ConfigMap:</p> <p><pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy operator namespace&gt;\n</code></pre> <pre><code>kubectl patch cm trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>To set the GitHub token used by Trivy add the <code>trivy.githubToken</code> value to the <code>trivy-operator-trivy-config</code> Secret:</p> <p><pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy opersator namespace&gt;\nGITHUB_TOKEN=&lt;your token&gt;\n</code></pre> <pre><code>kubectl patch secret trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n $GITHUB_TOKEN | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre></p> <p>The following table lists available settings with their default values. Check plugins' documentation to see configuration settings for common use cases. For example, switch Trivy from [Standalone] to ClientServer mode.</p>    CONFIGMAP KEY DEFAULT DESCRIPTION     <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>.   <code>vulnerabilityReports.scanJobsInSameNamespace</code> <code>\"false\"</code> Whether to run vulnerability scan jobs in same namespace of workload. Set <code>\"true\"</code> to enable.   <code>configAuditReports.scanner</code> <code>Trivy</code> The name of the plugin that generates config audit reports.   <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code>   <code>scanJob.nodeSelector</code> N/A JSON representation of the [nodeSelector] to be applied to the scanner pods so that they can run on nodes with matching labels. Example: <code>'{\"example.com/node-type\":\"worker\", \"cpu-type\": \"sandylake\"}'</code>   <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code>   <code>scanJob.templateLabel</code> N/A One-line comma-separated representation of the template labels which the user wants the scanner pods to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the scanner pods with the labels <code>foo: bar</code> and <code>env: stage</code>   <code>scanJob.podTemplatePodSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner pods to be secured with. Example: <code>{\"RunAsUser\": 1000, \"RunAsGroup\": 1000, \"RunAsNonRoot\": true}</code>   <code>scanJob.podTemplateContainerSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner containers (and their initContainers) to be amended with. Example: <code>{\"allowPrivilegeEscalation\": false, \"capabilities\": { \"drop\": [\"ALL\"]},\"privileged\": false, \"readOnlyRootFilesystem\": true }</code>   <code>compliance.failEntriesLimit</code> <code>\"10\"</code> Limit the number of fail entries per control check in the cluster compliance detail report.     <p>Tip</p> <p>You can delete a configuration key.For example, the following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key: <pre><code>TRIVY_OPERATOR_NAMESPACE=&lt;your trivy operator namespace&gt;\n</code></pre> <pre><code>kubectl patch cm trivy-operator-trivy-config -n $TRIVY_OPERATOR_NAMESPACE \\\n  --type json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre></p>","location":"settings/"},{"title":"Nsa 1.0","text":"<p>NSA, CISA Kubernetes Hardening Guidance v1.0 cybersecurity technical report is produced by trivy-operator and validate the following control checks :</p>    NAME DESCRIPTION KINDS     Non-root containers Check that container is not running as root Workload   Immutable container file systems Check that container root file system is immutable Workload   Preventing privileged containers Controls whether Pods can run privileged containers Workload   Share containers process namespaces Controls whether containers can share process namespaces Workload   Share host process namespaces Controls whether share host process namespaces Workload   Use the host network Controls whether containers can use the host network Workload   Run with root privileges or with root group membership Controls whether container applications can run with root privileges or with root group membership Workload   Restricts escalation to root privileges Control check restrictions escalation to root privileges Workload   Sets the SELinux context of the container Control checks if pod sets the SELinux context of the container Workload   Restrict a container's access to resources with AppArmor Control checks the restriction of containers access to resources with AppArmor Workload   Sets the seccomp profile used to sandbox containers Control checks the sets the seccomp profile used to sandbox containers Workload   Protecting Pod service account tokens Control check whether disable secret token been mount ,automountServiceAccountToken: false Node   Namespace kube-system should not be used by users Control check whether Namespace kube-system is not be used by users NetworkPolicy   Pod and/or namespace Selectors usage Control check validate the pod and/or namespace Selectors usage NetworkPolicy   Use CNI plugin that supports NetworkPolicy API Control check whether check cni plugin installed Node   Use ResourceQuota policies to limit resources Control check the use of ResourceQuota policy to limit aggregate resource usage within namespace ResourceQuota   Use LimitRange policies to limit resources Control check the use of LimitRange policy limit resource usage for namespaces or nodes LimitRange   Control plan disable insecure port Control check whether control plan disable insecure port Node   Encrypt etcd communication Control check whether etcd communication is encrypted Node   Ensure kube config file permission Control check whether kube config file permissions Node   Check that encryption resource has been set Control checks whether encryption resource has been set Node   Check encryption provider Control checks whether encryption provider has been set Node   Make sure anonymous-auth is unset Control checks whether anonymous-auth is unset Node   Make sure -authorization-mode=RBAC Control check whether RBAC permission is in use Node   Audit policy is configure Control check whether audit policy is configure Node   Audit log path is configure Control check whether audit log path is configure Node   Audit log aging Control check whether audit log aging is configure Node    <p>NSA, CISA Kubernetes Hardening Guidance v1.0 report will be generated every three hours by default.</p> <p>The NSA compliance report is composed of two parts :</p> <ul> <li> <p><code>spec</code>: represents the NSA compliance control checks specification, check details, and the mapping to the security scanner</p> </li> <li> <p><code>status</code>: represents the NSA compliance control checks results</p> </li> </ul> <p>Spec can be customized by amending the control checks <code>severity</code> or <code>cron</code> expression (report execution interval). As an example, let's enter <code>vi</code> edit mode and change the <code>cron</code> expression. <pre><code>kubectl edit compliance\n</code></pre> Once the report has been generated, you can fetch and review its results section. As an example, let's fetch the compliance status report in JSON format</p> <pre><code>kubectl get compliance nsa  -o=jsonpath='{.status}' | jq .\n</code></pre> <p>If failures are found in the NSA report and additional investigation is required, you can fetch the nsa-details report for advance investigation. As an example, let's fetch the report in JSON format <pre><code>kubectl get compliancedetail nsa-details -o json\n</code></pre></p>","location":"compliance/nsa-1.0/"},{"title":"Configuration Auditing","text":"","location":"configuration-auditing/"},{"title":"Setting","text":"<p>the following flags can be set with the <code>trivy-operator-trivy-cofnig</code> configmap in order to impact scanning</p>    CONFIGMAP KEY DEFAULT DESCRIPTION     <code>trivy.useBuiltinRegoPolicies</code> <code>true</code> The Flag to enable the usage of builtin rego policies by default   <code>trivy.supportedConfigAuditKinds</code> <code>Workload,Service,Role,ClusterRole,NetworkPolicy,Ingress,LimitRange,ResourceQuota</code> The Flag is the list of supported kinds separated by comma delimiter to be scanned by the config audit scanner    <p>As your organization deploys containerized workloads in Kubernetes environments, you will be faced with many configuration choices related to images, containers, control plane, and data plane. Setting these configurations improperly creates a high-impact security and compliance risk. DevOps, and platform owners need the ability to continuously assess build artifacts, workloads, and infrastructure against configuration hardening standards to remediate any violations.</p> <p>trivy-operator configuration audit capabilities are purpose-built for Kubernetes environments. In particular, trivy Operator continuously checks images, workloads, and Kubernetes infrastructure components against common configurations security standards and generates detailed assessment reports, which are then stored in the default Kubernetes database.</p> <p>Kubernetes applications and other core configuration objects, such as Ingress, NetworkPolicy and ResourceQuota resources, are evaluated against Built-in Policies.  Additionally, application and infrastructure owners can integrate these reports into incident response workflows for active remediation.</p>","location":"configuration-auditing/#setting"},{"title":"Built-in Configuration Audit Policies","text":"<p>The following sections list built-in configuration audit policies installed with trivy-operator. They are stored in the <code>trivy-operator-policies-config</code> ConfigMap created in the installation namespace (e.g. <code>trivy-system</code>). You can modify them or add a new policy. For example, follow the [Writing Custom Configuration Audit Policies] tutorial to add a custom policy that checks for recommended Kubernetes labels on any resource kind.</p>","location":"configuration-auditing/built-in-policies/"},{"title":"General","text":"NAME DESCRIPTION KINDS     CPU not limited Enforcing CPU limits prevents DoS via resource exhaustion. Workload   CPU requests not specified When containers have resource requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload   SYS_ADMIN capability added SYS_ADMIN gives the processes running inside the container privileges that are equivalent to root. Workload   Default capabilities not dropped The container should drop all default capabilities and add only those that are needed for its execution. Workload   Root file system is not read-only An immutable root file system prevents applications from writing to their local disk. This can limit intrusions, as attackers will not be able to tamper with the file system or write foreign executables to disk. Workload   Memory not limited Enforcing memory limits prevents DoS via resource exhaustion. Workload   Memory requests not specified When containers have memory requests specified, the scheduler can make better decisions about which nodes to place pods on, and how to deal with resource contention. Workload   hostPath volume mounted with docker.sock Mounting docker.sock from the host can give the container full root access to the host. Workload   Runs with low group ID Force the container to run with group ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload   Runs with low user ID Force the container to run with user ID &gt; 10000 to avoid conflicts with the host\u2019s user table. Workload   Tiller Is Deployed Check if Helm Tiller component is deployed. Workload   Image tag ':latest' used It is best to avoid using the ':latest' image tag when deploying containers in production. Doing so makes it hard to track which version of the image is running, and hard to roll back the version. Workload","location":"configuration-auditing/built-in-policies/#general"},{"title":"Advanced","text":"NAME DESCRIPTION KINDS     Unused capabilities should be dropped (drop any) Security best practices require containers to run with minimal required capabilities. Workload   hostAliases is set Managing /etc/hosts aliases can prevent the container engine from modifying the file after a pod\u2019s containers have already been started. Workload   User Pods should not be placed in kube-system namespace ensure that User pods are not placed in kube-system namespace Workload   Protecting Pod service account tokens ensure that Pod specifications disable the secret token being mounted by setting automountServiceAccountToken: false Workload   Selector usage in network policies ensure that network policies selectors are applied to pods or namespaces to restricted ingress and egress traffic within the pod network NetworkPolicy   limit range usage ensure limit range policy has configure in order to limit resource usage for namespaces or nodes LimitRange   resource quota usage ensure resource quota policy has configure in order to limit aggregate resource usage within namespace ResourceQuota   All container images must start with the *.azurecr.io domain Containers should only use images from trusted registries. Workload   All container images must start with a GCR domain Containers should only use images from trusted GCR registries. Workload","location":"configuration-auditing/built-in-policies/#advanced"},{"title":"Pod Security Standard","text":"","location":"configuration-auditing/built-in-policies/#pod-security-standard"},{"title":"Baseline","text":"NAME DESCRIPTION KINDS     Access to host IPC namespace Sharing the host\u2019s IPC namespace allows container processes to communicate with processes on the host. Workload   Access to host network Sharing the host\u2019s network namespace permits processes in the pod to communicate with processes bound to the host\u2019s loopback adapter. Workload   Access to host PID Sharing the host\u2019s PID namespace allows visibility on host processes, potentially leaking information such as environment variables and configuration. Workload   Privileged container Privileged containers share namespaces with the host system and do not offer any security. They should be used exclusively for system containers that require high privileges. Workload   Non-default capabilities added Adding NET_RAW or capabilities beyond the default set must be disallowed. Workload   hostPath volumes mounted HostPath volumes must be forbidden. Workload   Access to host ports HostPorts should be disallowed, or at minimum restricted to a known list. Workload   Default AppArmor profile not set A program inside the container can bypass AppArmor protection policies. Workload   SELinux custom options set Setting a custom SELinux user or role option should be forbidden. Workload   Non-default /proc masks set The default /proc masks are set up to reduce attack surface, and should be required. Workload   Unsafe sysctl options set Sysctls can disable security mechanisms or affect all containers on a host, and should be disallowed except for an allowed 'safe' subset. A sysctl is considered safe if it is namespaced in the container or the Pod, and it is isolated from other Pods or processes on the same Node. Workload","location":"configuration-auditing/built-in-policies/#baseline"},{"title":"Restricted","text":"NAME DESCRIPTION KINDS     Non-ephemeral volume types used In addition to restricting HostPath volumes, usage of non-ephemeral volume types should be limited to those defined through PersistentVolumes. Workload   Process can elevate its own privileges A program inside the container can elevate its own privileges and run as root, which might give the program control over the container and node. Workload   Runs as root user 'runAsNonRoot' forces the running image to run as a non-root user to ensure least privileges. Workload   A root primary or supplementary GID set Containers should be forbidden from running with a root primary or supplementary GID. Workload   Default Seccomp profile not set The RuntimeDefault seccomp profile must be required, or allow specific additional profiles. Workload","location":"configuration-auditing/built-in-policies/#restricted"},{"title":"Overview","text":"<p>This project houses CustomResourceDefinitions (CRDs) related to security and compliance checks along with the code generated by Kubernetes code generators to write such custom resources in a programmable way.</p>    NAME SHORTNAMES APIGROUP NAMESPACED KIND     vulnerabilityreports vulns,vuln aquasecurity.github.io true VulnerabilityReport   configauditreports configaudit,configaudits aquasecurity.github.io true ConfigAuditReport   exposedsecretsreports exposedsecret,exposedsecrets aquasecurity.github.io true ExposedSecretReport   rbacassessmentreports rbacassessmentreports,rbacassessmentreport aquasecurity.github.io true RbacAssessmentReport   clusterrbacassessmentreports clusterrbacassessmentreports,clusterrbacassessmentreport aquasecurity.github.io true ClusterRbacAssessmentReport","location":"crds/"},{"title":"ClusterComplianceReport","text":"<p>The ClusterComplianceReport is a cluster-scoped resource, which represents the latest compliance control checks results. The report spec defines a mapping between pre-defined compliance control check ids to security scanners check ids. Currently, only <code>kube-bench</code> and <code>config-audit</code> security scanners are supported.</p> <p>The NSA compliance report is composed of two parts:</p> <ul> <li><code>spec:</code> represents the compliance control checks specification, check details, and the mapping to the security scanner   (this part is defined by the user)</li> <li><code>status:</code> represents the compliance control checks (as defined by spec mapping) results extracted from the security   scanners reports (this part is output by trivy-operator)</li> </ul> <p>The following shows a sample ClusterComplianceReport NSA specification associated with the <code>cluster</code>:</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: ''\n  creationTimestamp: '2022-03-27T07:03:29Z'\n  generation: 2\n  labels:\n    app.kubernetes.io/instance: trivy-operator\n    app.kubernetes.io/managed-by: kubectl\n    app.kubernetes.io/name: trivy-operator\n    app.kubernetes.io/version: 0.4.0\n  name: nsa\n  resourceVersion: '15745'\n  uid: d11e8af1-daac-457d-96ea-45be4b043814\nspec:\n  controls:\n    - description: Check that container is not running as root\n      id: '1.0'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV012\n        scanner: config-audit\n      name: Non-root containers\n      severity: MEDIUM\n    - description: Check that container root file system is immutable\n      id: '1.1'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV014\n        scanner: config-audit\n      name: Immutable container file systems\n      severity: LOW\n    - description: Controls whether Pods can run privileged containers\n      id: '1.2'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV017\n        scanner: config-audit\n      name: Preventing privileged containers\n      severity: HIGH\n    - description: Controls whether containers can share process namespaces\n      id: '1.3'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV008\n        scanner: config-audit\n      name: Share containers process namespaces\n      severity: HIGH\n    - description: Controls whether share host process namespaces\n      id: '1.4'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV009\n        scanner: config-audit\n      name: Share host process namespaces.\n      severity: HIGH\n    - description: Controls whether containers can use the host network\n      id: '1.5'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV010\n        scanner: config-audit\n      name: use the host network\n      severity: HIGH\n    - description: Controls whether container applications can run with root privileges\n        or with root group membership\n      id: '1.6'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV029\n        scanner: config-audit\n      name: Run with root privileges or with root group membership\n      severity: LOW\n    - description: Control check restrictions escalation to root privileges\n      id: '1.7'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV001\n        scanner: config-audit\n      name: Restricts escalation to root privileges\n      severity: MEDIUM\n    - description: Control checks if pod sets the SELinux context of the container\n      id: '1.8'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV002\n        scanner: config-audit\n      name: Sets the SELinux context of the container\n      severity: MEDIUM\n    - description: Control checks the restriction of containers access to resources\n        with AppArmor\n      id: '1.9'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV030\n        scanner: config-audit\n      name: Restrict a container's access to resources with AppArmor\n      severity: MEDIUM\n    - description: Control checks the sets the seccomp profile used to sandbox containers\n      id: '1.10'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV030\n        scanner: config-audit\n      name: Sets the seccomp profile used to sandbox containers.\n      severity: LOW\n    - description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      id: '1.11'\n      kinds:\n        - Workload\n      mapping:\n        checks:\n          - id: KSV036\n        scanner: config-audit\n      name: Protecting Pod service account tokens\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check whether Namespace kube-system is not be used by users\n      id: '1.12'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        checks:\n          - id: KSV037\n        scanner: config-audit\n      name: Namespace kube-system should not be used by users\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check validate the pod and/or namespace Selectors usage\n      id: '2.0'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        checks:\n          - id: KSV038\n        scanner: config-audit\n      name: Pod and/or namespace Selectors usage\n      severity: MEDIUM\n    - description: \"Control check whether check cni plugin installed\\t\"\n      id: '3.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 5.3.1\n        scanner: kube-bench\n      name: Use CNI plugin that supports NetworkPolicy API\n      severity: CRITICAL\n    - defaultStatus: FAIL\n      description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      id: '4.0'\n      kinds:\n        - ResourceQuota\n      mapping:\n        checks:\n          - id: KSV040\n        scanner: config-audit\n      name: Use ResourceQuota policies to limit resources\n      severity: MEDIUM\n    - defaultStatus: FAIL\n      description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      id: '4.1'\n      kinds:\n        - ResourceQuota\n      mapping:\n        checks:\n          - id: KSV039\n        scanner: config-audit\n      name: Use LimitRange policies to limit resources\n      severity: MEDIUM\n    - description: Control check whether control plan disable insecure port\n      id: '5.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.19\n        scanner: kube-bench\n      name: Control plan disable insecure port\n      severity: CRITICAL\n    - description: Control check whether etcd communication is encrypted\n      id: '5.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: '2.1'\n        scanner: kube-bench\n      name: Encrypt etcd communication\n      severity: CRITICAL\n    - description: Control check whether kube config file permissions\n      id: '6.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 4.1.3\n          - id: 4.1.4\n        scanner: kube-bench\n      name: Ensure kube config file permission\n      severity: CRITICAL\n    - description: Control checks whether encryption resource has been set\n      id: '6.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.31\n          - id: 1.2.32\n        scanner: kube-bench\n      name: Check that encryption resource has been set\n      severity: CRITICAL\n    - description: Control checks whether encryption provider has been set\n      id: '6.2'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.3\n        scanner: kube-bench\n      name: Check encryption provider\n      severity: CRITICAL\n    - description: Control checks whether anonymous-auth is unset\n      id: '7.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.1\n        scanner: kube-bench\n      name: Make sure anonymous-auth is unset\n      severity: CRITICAL\n    - description: Control check whether RBAC permission is in use\n      id: '7.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.7\n          - id: 1.2.8\n        scanner: kube-bench\n      name: Make sure -authorization-mode=RBAC\n      severity: CRITICAL\n    - description: Control check whether audit policy is configure\n      id: '8.0'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 3.2.1\n        scanner: kube-bench\n      name: Audit policy is configure\n      severity: HIGH\n    - description: Control check whether audit log path is configure\n      id: '8.1'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.22\n        scanner: kube-bench\n      name: Audit log path is configure\n      severity: MEDIUM\n    - description: Control check whether audit log aging is configure\n      id: '8.2'\n      kinds:\n        - Node\n      mapping:\n        checks:\n          - id: 1.2.23\n        scanner: kube-bench\n      name: Audit log aging\n      severity: MEDIUM\n  cron: \"* * * * *\"\n  description: National Security Agency - Kubernetes Hardening Guidance\n  name: nsa\n  version: '1.0'\nstatus:\n  controlCheck:\n    - description: Controls whether Pods can run privileged containers\n      failTotal: 0\n      id: '1.2'\n      name: Preventing privileged containers\n      passTotal: 11\n      severity: HIGH\n    - description: Controls whether containers can share process namespaces\n      failTotal: 0\n      id: '1.3'\n      name: Share containers process namespaces\n      passTotal: 11\n      severity: HIGH\n    - description: Control checks whether anonymous-auth is unset\n      failTotal: 0\n      id: '7.0'\n      name: Make sure anonymous-auth is unset\n      passTotal: 0\n      severity: CRITICAL\n    - description: Control check restrictions escalation to root privileges\n      failTotal: 6\n      id: '1.7'\n      name: Restricts escalation to root privileges\n      passTotal: 5\n      severity: MEDIUM\n    - description: Control checks the restriction of containers access to resources\n        with AppArmor\n      failTotal: 0\n      id: '1.9'\n      name: Restrict a container's access to resources with AppArmor\n      passTotal: 11\n      severity: MEDIUM\n    - description: Check that container is not running as root\n      failTotal: 9\n      id: '1.0'\n      name: Non-root containers\n      passTotal: 2\n      severity: MEDIUM\n    - description: Controls whether share host process namespaces\n      failTotal: 0\n      id: '1.4'\n      name: Share host process namespaces.\n      passTotal: 11\n      severity: HIGH\n    - description: Control checks whether encryption resource has been set\n      failTotal: 0\n      id: '6.1'\n      name: Check that encryption resource has been set\n      passTotal: 1\n      severity: CRITICAL\n    - description: \"Control check whether check cni plugin installed\\t\"\n      failTotal: 0\n      id: '3.0'\n      name: Use CNI plugin that supports NetworkPolicy API\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      failTotal: 1\n      id: '4.0'\n      name: Use ResourceQuota policies to limit resources\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether kube config file permissions\n      failTotal: 0\n      id: '6.0'\n      name: Ensure kube config file permission\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control checks whether encryption provider has been set\n      failTotal: 0\n      id: '6.2'\n      name: Check encryption provider\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether RBAC permission is in use\n      failTotal: 0\n      id: '7.1'\n      name: Make sure -authorization-mode=RBAC\n      passTotal: 0\n      severity: CRITICAL\n    - description: Check that container root file system is immutable\n      failTotal: 5\n      id: '1.1'\n      name: Immutable container file systems\n      passTotal: 6\n      severity: LOW\n    - description: Control checks if pod sets the SELinux context of the container\n      failTotal: 0\n      id: '1.8'\n      name: Sets the SELinux context of the container\n      passTotal: 11\n      severity: MEDIUM\n    - description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      failTotal: 1\n      id: '1.11'\n      name: Protecting Pod service account tokens\n      passTotal: 10\n      severity: MEDIUM\n    - description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      failTotal: 1\n      id: '4.1'\n      name: Use LimitRange policies to limit resources\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether audit log aging is configure\n      failTotal: 0\n      id: '8.2'\n      name: Audit log aging\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether Namespace kube-system is not be used by users\n      failTotal: 8\n      id: '1.12'\n      name: Namespace kube-system should not be used by users\n      passTotal: 3\n      severity: MEDIUM\n    - description: Controls whether containers can use the host network\n      failTotal: 0\n      id: '1.5'\n      name: use the host network\n      passTotal: 11\n      severity: HIGH\n    - description: Controls whether container applications can run with root privileges\n        or with root group membership\n      failTotal: 1\n      id: '1.6'\n      name: Run with root privileges or with root group membership\n      passTotal: 10\n      severity: LOW\n    - description: Control check whether audit log path is configure\n      failTotal: 0\n      id: '8.1'\n      name: Audit log path is configure\n      passTotal: 1\n      severity: MEDIUM\n    - description: Control checks the sets the seccomp profile used to sandbox containers\n      failTotal: 0\n      id: '1.10'\n      name: Sets the seccomp profile used to sandbox containers.\n      passTotal: 11\n      severity: LOW\n    - description: Control check validate the pod and/or namespace Selectors usage\n      failTotal: 1\n      id: '2.0'\n      name: Pod and/or namespace Selectors usage\n      passTotal: 0\n      severity: MEDIUM\n    - description: Control check whether control plan disable insecure port\n      failTotal: 0\n      id: '5.0'\n      name: Control plan disable insecure port\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether etcd communication is encrypted\n      failTotal: 0\n      id: '5.1'\n      name: Encrypt etcd communication\n      passTotal: 1\n      severity: CRITICAL\n    - description: Control check whether audit policy is configure\n      failTotal: 0\n      id: '8.0'\n      name: Audit policy is configure\n      passTotal: 1\n      severity: HIGH\n  summary:\n    failCount: 33\n    passCount: 113\n  updateTimestamp: '2022-03-27T07:06:00Z'\n</code></pre>","location":"crds/clustercompliance-report/"},{"title":"ClusterComplianceDetailReport","text":"<p>The ClusterComplianceDetailReport is a cluster-scoped resource, which represents the latest result of the Cluster Compliance Detail report. The report data provide granular information on control checks failures that occur in <code>ClusterComplianceReport</code> for further investigation.</p> <p>The compliance detail report provides granular information insight on control check failures:</p> <ul> <li>Failing resource kind</li> <li>Name of the failing resource</li> <li>Namespace of the failing resource</li> <li>Failure error message</li> <li>Remediation</li> </ul> <p>The following listing shows a sample ClusterComplianceDetailReport for NSA specification associated with the <code>cluster</code></p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceDetailReport\nmetadata:\n  creationTimestamp: '2022-03-27T07:04:21Z'\n  generation: 6\n  name: nsa-details\n  resourceVersion: '15788'\n  uid: 9d36889d-086a-4fb3-b660-a3a3ecffe3c6\nreport:\n  controlCheck:\n    - checkResults:\n        - details:\n            - msg: ReplicaSet 'coredns-96cc4f57d' should not be set with 'kube-system' namespace\n              name: replicaset-coredns-96cc4f57d\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'coredns-5789895cd' should not be set with 'kube-system' namespace\n              name: replicaset-coredns-5789895cd\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'traefik-56c4b88c4b' should not be set with 'kube-system'\n                namespace\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'metrics-server-ff9dbcb6c' should not be set with 'kube-system'\n                namespace\n              name: replicaset-metrics-server-ff9dbcb6c\n              namespace: kube-system\n              status: FAIL\n            - msg: ReplicaSet 'local-path-provisioner-84bb864455' should not be set with\n                'kube-system' namespace\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: ReplicaSet\n        - details:\n            - msg: DaemonSet 'svclb-traefik' should not be set with 'kube-system' namespace\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: DaemonSet\n        - details:\n            - msg: Job 'helm-install-traefik-crd' should not be set with 'kube-system' namespace\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Job 'helm-install-traefik' should not be set with 'kube-system' namespace\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV037\n          objectType: Job\n      description: Control check whether Namespace kube-system is not be used by users\n      id: '1.12'\n      name: Namespace kube-system should not be used by users\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: ResourceQuota\n      description: Control check the use of ResourceQuota policy to limit aggregate\n        resource usage within namespace\n      id: '4.0'\n      name: Use ResourceQuota policies to limit resources\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'traefik' of ReplicaSet 'traefik-56c4b88c4b' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.allowPrivilegeEscalation' to false\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV001\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.allowPrivilegeEscalation'\n                to false\n              name: pod-nginx-jr99v\n              namespace: trivy-operator-itest\n              status: FAIL\n          id: KSV001\n          objectType: Pod\n      description: Control check restrictions escalation to root privileges\n      id: '1.7'\n      name: Restricts escalation to root privileges\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: ResourceQuota\n      description: Control check the use of LimitRange policy limit resource usage for\n        namespaces or nodes\n      id: '4.1'\n      name: Use LimitRange policies to limit resources\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.readOnlyRootFilesystem' to true\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV014\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.readOnlyRootFilesystem'\n                to true\n              name: pod-nginx-jr99v\n              namespace: trivy-operator-itest\n              status: FAIL\n          id: KSV014\n          objectType: Pod\n      description: Check that container root file system is immutable\n      id: '1.1'\n      name: Immutable container file systems\n      severity: LOW\n    - checkResults:\n        - details:\n            - msg: ReplicaSet 'traefik-56c4b88c4b' should set 'spec.securityContext.runAsGroup',\n                'spec.securityContext.supplementalGroups[*]' and 'spec.securityContext.fsGroup'\n                to integer greater than 0\n              name: replicaset-traefik-56c4b88c4b\n              namespace: kube-system\n              status: FAIL\n          id: KSV029\n          objectType: ReplicaSet\n      description: Controls whether container applications can run with root privileges\n        or with root group membership\n      id: '1.6'\n      name: Run with root privileges or with root group membership\n      severity: LOW\n    - checkResults:\n        - details:\n            - msg: Container of Pod 'nginx-jr99v' should set 'spec.automountServiceAccountToken'\n                to false\n              name: pod-nginx-jr99v\n              namespace: trivy-operator-itest\n              status: FAIL\n          id: KSV036\n          objectType: Pod\n      description: 'Control check whether disable secret token been mount ,automountServiceAccountToken:\n      false'\n      id: '1.11'\n      name: Protecting Pod service account tokens\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Resource do not exist in cluster\n              status: FAIL\n          objectType: NetworkPolicy\n      description: Control check validate the pod and/or namespace Selectors usage\n      id: '2.0'\n      name: Pod and/or namespace Selectors usage\n      severity: MEDIUM\n    - checkResults:\n        - details:\n            - msg: Container 'trivy-operator' of ReplicaSet 'trivy-operator-7cf866c47b'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-trivy-operator-7cf866c47b\n              namespace: trivy-system\n              status: FAIL\n            - msg: Container 'coredns' of ReplicaSet 'coredns-96cc4f57d' should set 'securityContext.runAsNonRoot'\n                to true\n              name: replicaset-coredns-96cc4f57d\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'coredns' of ReplicaSet 'coredns-5789895cd' should set 'securityContext.runAsNonRoot'\n                to true\n              name: replicaset-coredns-5789895cd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'trivy-operator' of ReplicaSet 'trivy-operator-c94dd56d'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-trivy-operator-c94dd56d\n              namespace: trivy-system\n              status: FAIL\n            - msg: Container 'local-path-provisioner' of ReplicaSet 'local-path-provisioner-84bb864455'\n                should set 'securityContext.runAsNonRoot' to true\n              name: replicaset-local-path-provisioner-84bb864455\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: ReplicaSet\n        - details:\n            - msg: Container 'lb-port-443' of DaemonSet 'svclb-traefik' should set 'securityContext.runAsNonRoot'\n                to true\n              name: daemonset-svclb-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: DaemonSet\n        - details:\n            - msg: Container 'helm' of Job 'helm-install-traefik-crd' should set 'securityContext.runAsNonRoot'\n                to true\n              name: job-helm-install-traefik-crd\n              namespace: kube-system\n              status: FAIL\n            - msg: Container 'helm' of Job 'helm-install-traefik' should set 'securityContext.runAsNonRoot'\n                to true\n              name: job-helm-install-traefik\n              namespace: kube-system\n              status: FAIL\n          id: KSV012\n          objectType: Job\n        - details:\n            - msg: Container 'nginx' of Pod 'nginx-jr99v' should set 'securityContext.runAsNonRoot'\n                to true\n              name: pod-nginx-jr99v\n              namespace: trivy-operator-itest\n              status: FAIL\n          id: KSV012\n          objectType: Pod\n      description: Check that container is not running as root\n      id: '1.0'\n      name: Non-root containers\n      severity: MEDIUM\n  summary:\n    failCount: 33\n    passCount: 113\n  type:\n    description: national security agency - kubernetes hardening guidance\n    name: nsa-details\n    version: '1.0'\n  updateTimestamp: '2022-03-27T07:09:00Z'\n</code></pre>","location":"crds/clustercompliancedetail-report/"},{"title":"ClusterConfigAuditReport","text":"<p>ClusterConfigAuditReport is equivalent to ConfigAuditReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings, and CustomResourceDefinitions.</p>","location":"crds/clusterconfigaudit-report/"},{"title":"ClusterRbacAssessmentReport","text":"<p>ClusterRbacAssessmentReport is equivalent to RbacAssessmentReport for cluster-scoped objects such as ClusterRoles, ClusterRoleBindings.</p>","location":"crds/clusterrbacassessment-report/"},{"title":"ConfigAuditReport","text":"<p>An instance of the ConfigAuditReport represents checks performed by configuration auditing tools, such as [Trivy],  against a Kubernetes object's configuration. For example, check that a given container image runs as non-root user or that a container has resource requests and limits set. Checks might relate to Kubernetes workloads and other namespaced Kubernetes objects such as Services, ConfigMaps, Roles, and RoleBindings.</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;workload-kind&gt;-&lt;workload-name&gt;</code> naming convention.</p> <p>The following listing shows a sample ConfigAuditReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6\n  namespace: default\n  labels:\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: nginx-6d4cf56db6\n    trivy-operator.resource.namespace: default\n    plugin-config-hash: 7f65d98b75\n    resource-spec-hash: 7cb64cb677\n  uid: d5cf8847-c96d-4534-beb9-514a34230302\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  updateTimestamp: '2021-05-20T12:38:10Z'\n  scanner:\n    name: Trivy \n    vendor: Aqua Security\n    version: '0.4.0'\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 9\n    mediumCount: 0\n  checks:\n    - category: Security\n      checkID: hostPIDSet\n      messages:\n        - Host PID is not configured\n      severity: CRITICAL\n      success: true\n    - category: Security\n      checkID: hostIPCSet\n      messages:\n        - Host IPC is not configured\n      severity: CRITICAL\n      success: true\n    - category: Security\n      checkID: hostNetworkSet\n      messages:\n        - Host network is not configured\n      severity: LOW\n      success: true\n    - category: Security\n      checkID: notReadOnlyRootFilesystem\n      messages:\n        - Filesystem should be read only\n      scope:\n        type: Container\n        value: nginx\n      severity: LOW\n      success: false\n    - category: Security\n      checkID: privilegeEscalationAllowed\n      messages:\n        - Privilege escalation should not be allowed\n      scope:\n        type: Container\n        value: nginx\n      severity: CRITICAL\n      success: false\n</code></pre> <p>Third party Kubernetes configuration checkers, linters, and sanitizers that are compliant with the ConfigAuditReport schema can be integrated with trivy-operator.</p>  <p>Note</p> <p>The challenge with onboarding third party configuration checkers is that they tend to have different interfaces to perform scans and vary in output formats for a relatively common goal, which is inspecting deployment descriptors for known configuration pitfalls.</p>","location":"crds/configaudit-report/"},{"title":"ExposedSecretReport","text":"<p>An instance of the ExposedSecretReport represents the secrets found in a container image of a given Kubernetes workload. It consists of a list exposed secrets with a summary grouped by severity. For a multi-container workload trivy-operator creates multiple instances of ExposedSecretsReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample ExposedSecretReport associated with the ReplicaSet named <code>app-574ddcb559</code> in the <code>default</code> namespace that has the <code>app</code> container.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ExposedSecretReport\nmetadata:\n  creationTimestamp: \"2022-06-29T14:25:54Z\"\n  generation: 2\n  labels:\n    resource-spec-hash: 8495697ff5\n    trivy-operator.container.name: app\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: app-67b77f5965\n    trivy-operator.resource.namespace: default\n  name: replicaset-app-67b77f5965-app\n  namespace: default\n  ownerReferences:\n  - apiVersion: apps/v1\n    blockOwnerDeletion: false\n    controller: true\n    kind: ReplicaSet\n    name: app-67b77f5965\n    uid: 04a744fe-1126-42d5-bb8b-0917bdb51a28\n  resourceVersion: \"1420\"\n  uid: 2b2697bb-d528-4d4d-8312-a74dcab6ac65\nreport:\n  artifact:\n    repository: myimagewithsecret\n    tag: v0.4.0\n  registry:\n    server: index.docker.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.31.3\n  secrets:\n  - category: Stripe\n    match: 'publishable_key: *****'\n    ruleID: stripe-access-token\n    severity: HIGH\n    target: \"\"\n    title: Stripe\n  - category: Stripe\n    match: 'secret_key: *****'\n    ruleID: stripe-access-token\n    severity: HIGH\n    target: \"\"\n    title: Stripe\n  summary:\n    criticalCount: 0\n    highCount: 2\n    lowCount: 0\n    mediumCount: 0\n  updateTimestamp: \"2022-06-29T14:29:37Z\"\n</code></pre>","location":"crds/exposedsecret-report/"},{"title":"RbacAssessmentReport","text":"<p>An instance of the RbacAssessmentReport represents checks performed by configuration auditing tools, such as [Trivy], against a Kubernetes rbac assessment. For example, check that a given Role do not expose permission to secret for all groups</p> <p>Each report is owned by the underlying Kubernetes object and is stored in the same namespace, following the <code>&lt;Role&gt;-&lt;role-name&gt;</code> naming convention.</p> <p>The following listing shows a sample RbacAssessmentReport associated with the Role named <code>role-868458b9d6</code> in the <code>default</code> namespace.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: RbacAssessmentReport\nmetadata:\n  annotations:\n    trivy-operator.resource.name: system:controller:token-cleaner\n  creationTimestamp: \"2022-07-04T07:23:07Z\"\n  generation: 1\n  labels:\n    plugin-config-hash: 659b7b9c46\n    resource-spec-hash: 59b6bf95c6\n    trivy-operator.resource.kind: Role\n    trivy-operator.resource.name-hash: 868458b9d6\n    trivy-operator.resource.namespace: default\n  name: role-868458b9d6\n  namespace: kube-system\n  ownerReferences:\n    - apiVersion: rbac.authorization.k8s.io/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: Role\n      name: system:controller:token-cleaner\n      uid: 44c6229b-d410-4bc5-9529-fff41de39d03\n  resourceVersion: \"7301\"\n  uid: 372d7c32-795f-4180-ae84-efd931efaf6e\nreport:\n  checks:\n    - category: Kubernetes Security Check\n      checkID: KSV051\n      description: Check whether role permits creating role bindings and associating\n        to privileged role/clusterrole\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow role binding creation and association with privileged role/clusterrole\n    - category: Kubernetes Security Check\n      checkID: KSV056\n      description: The ability to control which pods get service traffic directed to\n        them allows for interception attacks. Controlling network policy allows for\n        bypassing lateral movement restrictions.\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow management of networking resources\n    - category: Kubernetes Security Check\n      checkID: KSV041\n      description: Check whether role permits managing secrets\n      messages:\n        - Role permits management of secret(s)\n      severity: CRITICAL\n      success: false\n      title: Do not allow management of secrets\n    - category: Kubernetes Security Check\n      checkID: KSV047\n      description: Check whether role permits privilege escalation from node proxy\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow privilege escalation from node proxy\n    - category: Kubernetes Security Check\n      checkID: KSV045\n      description: Check whether role permits wildcard verb on specific resources\n      messages:\n        - \"\"\n      severity: CRITICAL\n      success: true\n      title: No wildcard verb roles\n    - category: Kubernetes Security Check\n      checkID: KSV054\n      description: Check whether role permits attaching to shell on pods\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow attaching to shell on pods\n    - category: Kubernetes Security Check\n      checkID: KSV044\n      description: Check whether role permits wildcard verb on wildcard resource\n      messages:\n        - \"\"\n      severity: CRITICAL\n      success: true\n      title: No wildcard verb and resource roles\n    - category: Kubernetes Security Check\n      checkID: KSV050\n      description: An effective level of access equivalent to cluster-admin should not\n        be provided.\n      messages:\n        - \"\"\n      severity: CRITICAL\n      success: true\n      title: Do not allow management of RBAC resources\n    - category: Kubernetes Security Check\n      checkID: KSV046\n      description: Check whether role permits specific verb on wildcard resources\n      messages:\n        - \"\"\n      severity: CRITICAL\n      success: true\n      title: No wildcard resource roles\n    - category: Kubernetes Security Check\n      checkID: KSV055\n      description: Check whether role permits allowing users in a rolebinding to add\n        other users to their rolebindings\n      messages:\n        - \"\"\n      severity: LOW\n      success: true\n      title: Do not allow users in a rolebinding to add other users to their rolebindings\n    - category: Kubernetes Security Check\n      checkID: KSV052\n      description: Check whether role permits creating role ClusterRoleBindings and\n        association with privileged cluster role\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow role to create ClusterRoleBindings and association with privileged\n        role\n    - category: Kubernetes Security Check\n      checkID: KSV053\n      description: Check whether role permits getting shell on pods\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow getting shell on pods\n    - category: Kubernetes Security Check\n      checkID: KSV042\n      description: Used to cover attacker\u2019s tracks, but most clusters ship logs quickly\n        off-cluster.\n      messages:\n        - \"\"\n      severity: MEDIUM\n      success: true\n      title: Do not allow deletion of pod logs\n    - category: Kubernetes Security Check\n      checkID: KSV049\n      description: Some workloads leverage configmaps to store sensitive data or configuration\n        parameters that affect runtime behavior that can be modified by an attacker\n        or combined with another issue to potentially lead to compromise.\n      messages:\n        - \"\"\n      severity: MEDIUM\n      success: true\n      title: Do not allow management of configmaps\n    - category: Kubernetes Security Check\n      checkID: KSV043\n      description: Check whether role permits impersonating privileged groups\n      messages:\n        - \"\"\n      severity: CRITICAL\n      success: true\n      title: Do not allow impersonation of privileged groups\n    - category: Kubernetes Security Check\n      checkID: KSV048\n      description: Check whether role permits update/create of a malicious pod\n      messages:\n        - \"\"\n      severity: HIGH\n      success: true\n      title: Do not allow update/create of a malicious pod\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: '0.4.0'\n  summary:\n    criticalCount: 1\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n  updateTimestamp: null\n</code></pre>","location":"crds/rbacassessment-report/"},{"title":"VulnerabilityReport","text":"<p>An instance of the VulnerabilityReport represents the latest vulnerabilities found in a container image of a given Kubernetes workload. It consists of a list of OS package and application vulnerabilities with a summary of vulnerabilities grouped by severity. For a multi-container workload trivy-operator creates multiple instances of VulnerabilityReports in the workload's namespace with the owner reference set to that workload. Each report follows the naming convention <code>&lt;workload kind&gt;-&lt;workload name&gt;-&lt;container-name&gt;</code>.</p> <p>The following listing shows a sample VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container without any additional options.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6-nginx\n  namespace: default\n  labels:\n    trivy-operator.container.name: nginx\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: nginx-6d4cf56db6\n    trivy-operator.resource.namespace: default\n    resource-spec-hash: 7cb64cb677\n  uid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  artifact:\n    repository: library/nginx\n    tag: '1.16'\n  registry:\n    server: index.docker.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.31.3\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n    - fixedVersion: 0.9.1-2+deb10u1\n      installedVersion: 0.9.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2019-20367'\n      resource: libbsd0\n      score: 9.1\n      severity: CRITICAL\n      target: library/nginx:1.21.6\n      title: ''\n      vulnerabilityID: CVE-2019-20367\n    - fixedVersion: ''\n      installedVersion: 0.6.1-2\n      links: []\n      primaryLink: 'https://avd.aquasec.com/nvd/cve-2018-25009'\n      resource: libwebp6\n      score: 9.1\n      severity: CRITICAL\n      target: library/nginx:1.16\n      title: 'libwebp: out-of-bounds read in WebPMuxCreateInternal'\n      vulnerabilityID: CVE-2018-25009\n</code></pre>  <p>Note</p> <p>For various reasons we'll probably change the naming convention to name VulnerabilityReports by image digest (see #288).</p>  <p>Any static vulnerability scanner that is compliant with the VulnerabilityReport schema can be integrated with trivy-operator. You can find the list of available integrations here.</p> <p>It's possible to get more information from report, like Description, Links, CVSS and Target. The following listing shows a sample of extended VulnerabilityReport associated with the ReplicaSet named <code>nginx-6d4cf56db6</code> in the <code>default</code> namespace that has the <code>nginx</code> container with additional options. Please refer to the \"Vulnerability Scanner Configuration\" how to make it. Use with caution, because Links can generate lots of information and report can exceed the etcd request payload limit. By default, the payload of each Kubernetes object stored etcd is subject to 1.5 MiB.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  name: replicaset-nginx-6d4cf56db6-nginx\n  namespace: default\n  labels:\n    trivy-operator.container.name: nginx\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: nginx-6d4cf56db6\n    trivy-operator.resource.namespace: default\n    resource-spec-hash: 7cb64cb677\n  uid: 8aa1a7cb-a319-4b93-850d-5a67827dfbbf\n  ownerReferences:\n    - apiVersion: apps/v1\n      blockOwnerDeletion: false\n      controller: true\n      kind: ReplicaSet\n      name: nginx-6d4cf56db6\n      uid: aa345200-cf24-443a-8f11-ddb438ff8659\nreport:\n  artifact:\n    repository: library/nginx\n    tag: '1.16'\n  registry:\n    server: index.docker.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.31.3\n  summary:\n    criticalCount: 2\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  vulnerabilities:\n   - cvss:\n      nvd:\n        V2Score: 4.6\n        V2Vector: AV:L/AC:L/Au:N/C:P/I:P/A:P\n        V3Score: 5.7\n        V3Vector: CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:L/I:L/A:L\n      redhat:\n        V3Score: 5.7\n        V3Vector: CVSS:3.1/AV:L/AC:L/PR:H/UI:N/S:C/C:L/I:L/A:L\n    description: 'APT had several integer overflows and underflows while parsing .deb\n      packages, aka GHSL-2020-168 GHSL-2020-169, in files apt-pkg/contrib/extracttar.cc,\n      apt-pkg/deb/debfile.cc, and apt-pkg/contrib/arfile.cc. This issue affects: apt\n      1.2.32ubuntu0 versions prior to 1.2.32ubuntu0.2; 1.6.12ubuntu0 versions prior\n      to 1.6.12ubuntu0.2; 2.0.2ubuntu0 versions prior to 2.0.2ubuntu0.2; 2.1.10ubuntu0\n      versions prior to 2.1.10ubuntu0.1;'\n    fixedVersion: 1.8.2.2\n    installedVersion: 1.8.2\n    links:\n    - https://access.redhat.com/security/cve/CVE-2020-27350\n    - https://bugs.launchpad.net/bugs/1899193\n    - https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-27350\n    - https://security.netapp.com/advisory/ntap-20210108-0005/\n    - https://ubuntu.com/security/notices/USN-4667-1\n    - https://ubuntu.com/security/notices/USN-4667-2\n    - https://usn.ubuntu.com/usn/usn-4667-1\n    - https://www.debian.org/security/2020/dsa-480\n    primaryLink: https://avd.aquasec.com/nvd/cve-2020-27350\n    resource: apt\n    severity: MEDIUM\n    target: nginx:1.16 (debian 10.3)\n    title: 'apt: integer overflows and underflows while parsing .deb packages'\n    vulnerabilityID: CVE-2020-27350\n  - cvss:\n      nvd:\n        V2Score: 4.3\n        V2Vector: AV:N/AC:M/Au:N/C:N/I:N/A:P\n        V3Score: 5.5\n        V3Vector: CVSS:3.1/AV:L/AC:L/PR:N/UI:R/S:U/C:N/I:N/A:H\n    description: Missing input validation in the ar/tar implementations of APT before\n      version 2.1.2 could result in denial of service when processing specially crafted\n      deb files.\n    fixedVersion: 1.8.2.1\n    installedVersion: 1.8.2\n    links:\n    - https://bugs.launchpad.net/bugs/1878177\",\n    - https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-3810\n    - https://github.com/Debian/apt/issues/111\n    - https://github.com/julian-klode/apt/commit/de4efadc3c92e26d37272fd310be148ec61dcf36\n    - https://lists.debian.org/debian-security-announce/2020/msg00089.html\n    - https://lists.fedoraproject.org/archives/list/package-announce@lists.fedoraproject.org/message/U4PEH357MZM2SUGKETMEHMSGQS652QHH/\n    - https://salsa.debian.org/apt-team/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6\n    - https://salsa.debian.org/jak/apt/-/commit/dceb1e49e4b8e4dadaf056be34088b415939cda6\n    - https://tracker.debian.org/news/1144109/accepted-apt-212-source-into-unstable/\n    - https://ubuntu.com/security/notices/USN-4359-1\n    - https://ubuntu.com/security/notices/USN-4359-2\n    - https://usn.ubuntu.com/4359-1/\n    - https://usn.ubuntu.com/4359-2/\n    primaryLink: https://avd.aquasec.com/nvd/cve-2020-3810\n    resource: apt\n    severity: MEDIUM\n    target: nginx:1.16 (debian 10.3)\n    title: Missing input validation in the ar/tar implementations of APT before v\n      ...\n    vulnerabilityID: CVE-2020-3810\n</code></pre>","location":"crds/vulnerability-report/"},{"title":"Design Documents","text":"<p>An index of various (informal) design and explanation documents that were created for different purposes. Mainly to brainstorm how trivy-operator works.</p>  <p>NOTE This is not an official documentation of trivy-operator. Some design documents may be out of date.</p>","location":"design/"},{"title":"Overview","text":"File Description     caching_scan_result_by_repo_digest.md [DRAFT] Caching Scan Results by Image Reference   design_trivy_file_system_scanner.md Scan Container Images with Trivy Filesystem Scanner   design_vulnerability_scan_in_same_ns.md Schedule vulnerability scan jobs in the same namespace as scanned workload   design_scan_by_image_digest.png Design of vulnerability scanning by image digest (ContainerStatus vs PodSpec).   design_vulnerability_scanning_2.0.png Design of most efficient vulnerability scanning that you can imagine.   design_namespace_security_report.pdf Design of a security report generated by trivy-operator CLI for a given namespace.","location":"design/#overview"},{"title":"[DRAFT] Caching Scan Results by Image Reference","text":"","location":"design/caching_scan_results_by_repo_digest/"},{"title":"TL;DR;","text":"<p>To find vulnerabilities in container images Trivy-Operator creates asynchronous Kubernetes (K8s) Jobs. Even though running a vulnerability scanner as a K8s Job is expensive, Trivy-Operator does not reuse scan results in any way. For example, if a workload refers to the image that has already been scanned, Trivy-Operator will go ahead and create another (similar) K8s Job.</p> <p>To some extent, the problem of wasteful and long-running K8s Jobs can be mitigated by using Trivy-Operator with Trivy in the ClientServer mode instead of the default Standalone mode. In this case a configured Trivy server will cache results of scanning image layers. However, there is still unnecessary overhead for managing K8s Jobs and communication between Trivy client and server. (The only real difference is that some Jobs may complete faster for already scanned images.)</p> <p>To solve the above-mentioned problems, we could cache scan results by image reference. For example, a CRD based implementation can store scan results as instances of ClusterVulnerabilityReport object named after a hash of the repo digest. An alternative implementation may cache vulnerability reports in an AWS S3 bucket or a similar key-value store.</p>","location":"design/caching_scan_results_by_repo_digest/#tldr"},{"title":"Example","text":"<p>With the proposed cluster-scoped (or global) cache, Trivy-Operator can check if the image with the specified reference has already been scanned. If yes, it will just read the corresponding ClusterVulnerabilityReport, copy its payload, and finally create an instance of a namespaced VulnerabilityReport.</p> <p>Let's consider two <code>nginx:1.16</code> Deployments in two different namespaces <code>foo</code> and <code>bar</code>. In the current implementation Trivy-Operator will spin up two K8s Jobs to run a scanner and eventually create two VulnerabilityReports in <code>foo</code> and <code>bar</code> namespaces respectively.</p> <p>In a cluster where Trivy-Operator is installed for the first time, when we scan the <code>nginx</code> Deployment in the <code>foo</code> namespace there's obviously no ClusterVulnerabilityReport for <code>nginx:1.16</code>. Therefore, Trivy-Operator will spin up a K8s Job and wait for its completion. On completion, it will create a cluster-scoped ClusterVulnerabilityReport named after the hash of <code>nginx:1.16</code>. It will also create a namespaced VulnerabilityReport named after the current revision of the <code>nginx</code> Deployment.</p>  <p>NOTE Because a repo digest is not a valid name for a K8s API object, we may, for example, calculate a (safe) hash of the repo digest and use is as name instead.</p>  <pre><code>$ kubectl get clustervulnerabilityreports\nNo resources found\n</code></pre> <pre><code>$ trivy-operator scan vulnerabilityreports deploy/nginx -n foo -v 3\nI1008 19:58:19.355462   62385 scanner.go:72] Getting Pod template for workload: {Deployment nginx foo}\nI1008 19:58:19.358802   62385 scanner.go:89] Checking if images were already scanned\nI1008 19:58:19.360411   62385 scanner.go:95] Cached scan reports: 0\nI1008 19:58:19.360421   62385 scanner.go:101] Scanning with options: {ScanJobTimeout:0s DeleteScanJob:true}\nI1008 19:58:19.365155   62385 runner.go:79] Running task and waiting forever\nI1008 19:58:19.365190   62385 runnable_job.go:74] Creating job \"trivy-operator/scan-vulnerabilityreport-cbf8c9b99\"\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Event (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376920   62385 reflector.go:255] Listing and watching *v1.Event from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376902   62385 reflector.go:219] Starting reflector *v1.Job (30m0s) from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.376937   62385 reflector.go:255] Listing and watching *v1.Job from pkg/mod/k8s.io/client-go@v0.22.2/tools/cache/reflector.go:167\nI1008 19:58:19.386049   62385 runnable_job.go:130] Event: Created pod: scan-vulnerabilityreport-cbf8c9b99-4nzkb (SuccessfulCreate)\nI1008 19:58:51.243554   62385 runnable_job.go:130] Event: Job completed (Completed)\nI1008 19:58:51.247251   62385 runnable_job.go:109] Stopping runnable job on task completion with status: Complete\nI1008 19:58:51.247273   62385 runner.go:83] Stopping runner on task completion with error: &lt;nil&gt;\nI1008 19:58:51.247278   62385 scanner.go:130] Scan job completed: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.247297   62385 scanner.go:262] Getting logs for nginx container in job: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\nI1008 19:58:51.674449   62385 scanner.go:123] Deleting scan job: trivy-operator/scan-vulnerabilityreport-cbf8c9b99\n</code></pre> <p>Now, if we scan the <code>nginx</code> Deployment in the <code>bar</code> namespace, Trivy-Operator will see that there's already a ClusterVulnerabilityReport (<code>84bcb5cd46</code>) for the same image reference <code>nginx:1.16</code> and will skip creation of a K8s Job. It will just read and copy the report as VulnerabilityReport object to the <code>bar</code> namespace.</p> <pre><code>$ kubectl get clustervulnerabilityreports -o wide\nNAME         REPOSITORY      TAG    DIGEST   SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\n84bcb5cd46   library/nginx   1.16            Trivy     17s   21         50     33       104   0\n</code></pre> <pre><code>$ trivy-operator scan vulnerabilityreports deploy/nginx -n bar -v 3\nI1008 19:59:23.891718   62478 scanner.go:72] Getting Pod template for workload: {Deployment nginx bar}\nI1008 19:59:23.895310   62478 scanner.go:89] Checking if image nginx:1.16 was already scanned\nI1008 19:59:23.903058   62478 scanner.go:95] Cache hit\nI1008 19:59:23.903078   62478 scanner.go:97] Copying ClusterVulnerabilityReport to VulnerabilityReport\n</code></pre> <p>As you can see, Trivy-Operator eventually created two VulnerabilityReports by spinning up only one K8s Job.</p> <pre><code>$ kubectl get vulnerabilityreports -A\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     5m38s\nfoo         replicaset-nginx-6d4cf56db6-nginx   library/nginx   1.16   Trivy     6m10s\n</code></pre>","location":"design/caching_scan_results_by_repo_digest/#example"},{"title":"Life-cycle management","text":"<p>Just like any other cache it's very important that it's up to date and contains the correct information. To make sure of this we need to have a automated way of automatically cleaning up the ClusterVulnerabilityReport after some time.</p> <p>My suggestion is to solve this problem just like we did in PR #879. For each ClusterVulnerabilityReport created we should annotate the report with <code>trivy-operator.aquasecurity.github.io/cluster-vulnerability-report-ttl</code>. When the TTL ends the other controller will automatically delete the existing ClusterVulnerabilityReport and the next time the image is created in the cluster and normal vulnerabilityreport scan will happen.</p> <p>I suggest that we have a default value of 72 hours for this report. This is a new feature and I don't see why we shouldn't enable it by default.</p>","location":"design/caching_scan_results_by_repo_digest/#life-cycle-management"},{"title":"Vulnerability reports","text":"<p>From a vulnerability reports point of view we need to have a simple way for cluster admins to know if the vulnerability report is generated from a cache and if so which one?</p> <p>We could ether do this by setting a status on the vulnerability report that gets created but since this feature won't be on by default I suggest we use annotations.</p> <p>For example: <code>trivy-operator.aquasecurity.github.io/ClusterVulnerabilityReportName: 84bcb5cd46</code> would make it easy to find. We can't use something like ownerReference since it would delete all vulnerabilities at the same time if a ClusterVulnerabilityReport was deleted.</p>","location":"design/caching_scan_results_by_repo_digest/#vulnerability-reports"},{"title":"Summary","text":"<ul> <li>This solution might be the first step towards more efficient vulnerability scanning.</li> <li>It's backward compatible and can be implemented as an experimental feature behind   a gate.</li> <li>Both Trivy-Operator CLI and Trivy-Operator Operator can read and leverage ClusterVulnerabilityReports.</li> </ul>","location":"design/caching_scan_results_by_repo_digest/#summary"},{"title":"Support Compliance Reports","text":"","location":"design/design_compliance_report/"},{"title":"Overview","text":"<p>It is required to leverage trivy-operator security tools capabilities by adding the support for building compliance reports example : NSA - Kubernetes Hardening Guidance</p>","location":"design/design_compliance_report/#overview"},{"title":"Solution","text":"","location":"design/design_compliance_report/#solution"},{"title":"TL;DR;","text":"<ul> <li>A cluster compliance resource ,nsa-1.0.yaml (example below), with spec definition only will be deployed to kubernetes cluster upon startup</li> <li>the spec definition wil include the control check , cron expression for periodical generation, and it's mapping to scanners (kube-bench and audit-config)</li> <li>a new cluster compliance reconcile loop wil be introduced to track this cluster compliance resource </li> <li>when the cluster spec is reconcile  it check if cron expression match current time , if so it generates a compliance report and update the status section with report data</li> <li>if cron expression do not match the event will be requeue until next generation time </li> <li>Two new CRDs will be introduced :</li> <li><code>ClusterComplianceReport</code> to provide summary of the compliance per control</li> <li><code>ClusterComplianceDetailReport</code> to provide more detail compliance report for further investigation</li> <li>It is assumed that all scanners (kube-bench / config-audit) are running by default all the time and producing raw data</li> </ul>","location":"design/design_compliance_report/#tldr"},{"title":"The Spec file :","text":"<ul> <li>The spec will include the mapping (based on Ids) between the compliance report and tools(kube-bench and config-audit) which generate the raw data</li> <li>The spec file will be loaded from the file system</li> </ul>","location":"design/design_compliance_report/#the-spec-file"},{"title":"Example for spec  :","text":"<pre><code>---\napiVersion: aquasecurity.github.io/v1alpha1\nkind: ClusterComplianceReport\nmetadata:\n  name: nsa\nspec:\n  name: nsa\n  description: National Security Agency - Kubernetes Hardening Guidance\n  version: \"1.0\"\n  cron: \"0 */3 * * *\"\n  controls:\n    - name: Non-root containers\n      description: 'Check that container is not running as root'\n      id: '1.0'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV012\n      severity: 'MEDIUM'\n    - name: Immutable container file systems\n      description: 'Check that container root file system is immutable'\n      id: '1.1'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV014\n      severity: 'LOW'\n    - name: Preventing privileged containers\n      description: 'Controls whether Pods can run privileged containers'\n      id: '1.2'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV017\n      severity: 'HIGH'\n    - name: Share containers process namespaces\n      description: 'Controls whether containers can share process namespaces'\n      id: '1.3'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV008\n      severity: 'HIGH'\n    - name: Share host process namespaces.\n      description: 'Controls whether share host process namespaces'\n      id: '1.4'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV009\n      severity: 'HIGH'\n    - name: use the host network\n      description: 'Controls whether containers can use the host network'\n      id: '1.5'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV010\n      severity: 'HIGH'\n    - name:  Run with root privileges or with root group membership\n      description: 'Controls whether container applications can run with root privileges or with root group membership'\n      id: '1.6'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV029\n      severity: 'LOW'\n    - name: Restricts escalation to root privileges\n      description: 'Control check restrictions escalation to root privileges'\n      id: '1.7'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV001\n      severity: 'MEDIUM'\n    - name: Sets the SELinux context of the container\n      description: 'Control checks if pod sets the SELinux context of the container'\n      id: '1.8'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV002\n      severity: 'MEDIUM'\n    - name: Restrict a container's access to resources with AppArmor\n      description: 'Control checks the restriction of containers access to resources with AppArmor'\n      id: '1.9'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV030\n      severity: 'MEDIUM'\n    - name: Sets the seccomp profile used to sandbox containers.\n      description: 'Control checks the sets the seccomp profile used to sandbox containers'\n      id: '1.10'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV030\n      severity: 'LOW'\n    - name: Protecting Pod service account tokens\n      description: 'Control check whether disable secret token been mount ,automountServiceAccountToken: false'\n      id: '1.11'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV036\n      severity: 'MEDIUM'\n    - name: Namespace kube-system should not be used by users\n      description: 'Control check whether Namespace kube-system is not be used by users'\n      id: '1.12'\n      kinds:\n        - NetworkPolicy\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV037\n      severity: 'MEDIUM'\n    - name: Pod and/or namespace Selectors usage\n      description: 'Control check validate the pod and/or namespace Selectors usage'\n      id: '2.0'\n      kinds:\n        - Workload\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: KSV038\n      severity: 'MEDIUM'\n    - name: Use CNI plugin that supports NetworkPolicy API\n      description: 'Control check whether check cni plugin installed    '\n      id: '3.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 5.3.1\n      severity: 'CRITICAL'\n    - name: Use ResourceQuota policies to limit resources\n      description: 'Control check the use of ResourceQuota policies to limit resources'\n      id: '4.0'\n      kinds:\n        - ResourceQuota\n      mapping:\n        scanner: config-audit\n        checks:\n          - id: \"&lt;check need to be added&gt;\"\n      severity: 'CRITICAL'\n    - name: Control plan disable insecure port\n      description: 'Control check whether control plan disable insecure port'\n      id: '5.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.19\n      severity: 'CRITICAL'\n    - name: Encrypt etcd communication\n      description: 'Control check whether etcd communication is encrypted'\n      id: '5.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: '2.1'\n      severity: 'CRITICAL'\n    - name: Ensure kube config file permission\n      description: 'Control check whether kube config file permissions'\n      id: '6.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 4.1.3\n          - id: 4.1.4\n      severity: 'CRITICAL'\n    - name: Check that encryption resource has been set\n      description: 'Control checks whether encryption resource has been set'\n      id: '6.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.31\n          - id: 1.2.32\n      severity: 'CRITICAL'\n    - name: Check encryption provider\n      description: 'Control checks whether encryption provider has been set'\n      id: '6.2'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.3\n      severity: 'CRITICAL'\n    - name: Make sure anonymous-auth is unset\n      description: 'Control checks whether anonymous-auth is unset'\n      id: '7.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.1\n      severity: 'CRITICAL'\n    - name: Make sure -authorization-mode=RBAC\n      description: 'Control check whether RBAC permission is in use'\n      id: '7.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.7\n          - id: 1.2.8\n      severity: 'CRITICAL'\n    - name: Audit policy is configure\n      description: 'Control check whether audit policy is configure'\n      id: '8.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 3.2.1\n      severity: 'HIGH'\n    - name: Audit log path is configure\n      description: 'Control check whether audit log path is configure'\n      id: '8.1'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.22\n      severity: 'MEDIUM'\n    - name: Audit log aging\n      description: 'Control check whether audit log aging is configure'\n      id: '8.2'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: 1.2.23\n      severity: 'MEDIUM'\n    - name: Service mesh is configure\n      description: 'Control check whether service mesh is used in cluster'\n      id: '9.0'\n      kinds:\n        - Node\n      mapping:\n        scanner: kube-bench\n        checks:\n          - id: \"&lt;check need to be added&gt;\"\n      severity: 'MEDIUM'\n  ....\n ``` \n### The logic :\nUpon trivy-operator start cluster compliance reconcile loop will track the deployed spec file ,nsa-1.0 spec and evaluation the cron expression in spec file, \nif  the cron interval matches , trivy-operator will generate the compliance and compliance detail reports :\n -  `ClusterComplianceReport` status section will be updated with report data \n - `ClusterComplianceDetailReport` will be generated by and saved to etcd\n\n### The mapping\nOnce it is determined that a report need to be generated:\n- all reports (cis-benchmark and audit config) raw data will be fetched by `tool` and `resource` types\n- trivy-operator will iterate all fetched raw data and find a match by `ID`\n- once the data has been mapped and aggregated 2 type of reports will be generated to present summary\n  data and detailed data (in case further investigation need to be made)\n\n### Note: once the report has been generated again to reconcile loop start again the process describe in logic\n\n### The Reports:\n\n#### Example: Compliance spec and status section (report data)\n```json\n{\n  \"kind\": \"ClusterComplianceReport\",\n  \"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n  \"metadata\": {\n    \"name\": \"nsa\",\n    \"resourceVersion\": \"1000\",\n    \"creationTimestamp\": null\n  },\n  \"spec\": {\n    \"kind\": \"compliance\",\n    \"name\": \"nsa\",\n    \"description\": \"National Security Agency - Kubernetes Hardening Guidance\",\n    \"cron\": \"* * * * *\",\n    \"version\": \"1.0\",\n    \"controls\": [\n      {\n        \"id\": \"1.0\",\n        \"name\": \"Non-root containers\",\n        \"description\": \"\",\n        \"resources\": [\n          \"Workload\"\n        ],\n        \"mapping\": {\n          \"tool\": \"config-audit\",\n          \"checks\": [\n            {\n              \"id\": \"KSV012\"\n            }\n          ]\n        }\n      },\n      {\n        \"id\": \"8.2\",\n        \"name\": \"Audit log aging\",\n        \"description\": \"\",\n        \"resources\": [\n          \"Node\"\n        ],\n        \"mapping\": {\n          \"tool\": \"kube-bench\",\n          \"checks\": [\n            {\n              \"id\": \"1.2.23\"\n            }\n          ]\n        }\n      }\n    ]\n  },\n  \"status\": {\n    \"updateTimestamp\": \"2022-02-26T14:11:39Z\",\n    \"summary\": {\n      \"passCount\": 3,\n      \"failCount\": 3\n    },\n    \"control_check\": [\n      {\n        \"id\": \"1.1\",\n        \"name\": \"Immutable container file systems\",\n        \"passTotal\": 0,\n        \"failTotal\": 3,\n        \"severity\": \"\"\n      }\n    ]\n  }\n}\n</code></pre>","location":"design/design_compliance_report/#example-for-spec"},{"title":"Compliance details report","text":"<pre><code>{\n  \"kind\": \"ClusterComplianceDetailReport\",\n  \"apiVersion\": \"aquasecurity.github.io/v1alpha1\",\n  \"metadata\": {\n    \"name\": \"nsa-details\",\n    \"resourceVersion\": \"1\"\n  },\n  \"report\": {\n    \"updateTimestamp\": \"2022-02-26T14:05:29Z\",\n    \"type\": {\n      \"kind\": \"compliance\",\n      \"name\": \"nsa-details\",\n      \"description\": \"national security agency - kubernetes hardening guidance\",\n      \"version\": \"1.0\"\n    },\n    \"summary\": {\n      \"passCount\": 3,\n      \"failCount\": 3\n    },\n    \"controlCheck\": [\n      {\n        \"id\": \"1.1\",\n        \"name\": \"Immutable container file systems\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Pod\",\n            \"id\": \"KSV014\",\n            \"remediation\": \"\",\n            \"details\": [\n              {\n                \"name\": \"pod-rss-site\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'front-end' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              },\n              {\n                \"name\": \"pod-rss-site\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'rss-reader' of Pod 'rss-site' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              }\n            ]\n          },\n          {\n            \"objectType\": \"ReplicaSet\",\n            \"id\": \"KSV014\",\n            \"remediation\": \"\",\n            \"details\": [\n              {\n                \"name\": \"replicaset-memcached-sample-6c765df685\",\n                \"namespace\": \"default\",\n                \"msg\": \"Container 'memcached' of ReplicaSet 'memcached-sample-6c765df685' should set 'securityContext.readOnlyRootFilesystem' to true\",\n                \"status\": \"fail\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"3.0\",\n        \"name\": \"Use CNI plugin that supports NetworkPolicy API\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"5.3.1\",\n            \"remediation\": \"If the CNI plugin in use does not support network policies, consideration should be given to\\nmaking use of a different plugin, or finding an alternate mechanism for restricting traffic\\nin the Kubernetes cluster.\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"warn\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"6.0\",\n        \"name\": \"Ensure kube config file permission\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"4.1.3\",\n            \"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example,\\nchmod 644 /etc/kubernetes/proxy.conf\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"pass\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"id\": \"6.0\",\n        \"name\": \"Ensure kube config file permission\",\n        \"checkResults\": [\n          {\n            \"objectType\": \"Node\",\n            \"id\": \"4.1.4\",\n            \"remediation\": \"Run the below command (based on the file location on your system) on the each worker node.\\nFor example, chown root:root /etc/kubernetes/proxy.conf\\n\",\n            \"details\": [\n              {\n                \"name\": \"local-control-plane\",\n                \"namespace\": \"\",\n                \"msg\": \"\",\n                \"status\": \"pass\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>","location":"design/design_compliance_report/#compliance-details-report"},{"title":"The CRDs","text":"","location":"design/design_compliance_report/#the-crds"},{"title":"ClusterComplianceReport CRD :","text":"<ul> <li>a new CRD <code>clustercompliancereports.crd.yaml</code> will be added to include compliance check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: clustercompliancereports.aquasecurity.github.io\n  labels:\n    app.kubernetes.io/managed-by: trivy-operator\n    app.kubernetes.io/version: \"0.14.1\"\nspec:\n  group: aquasecurity.github.io\n  scope: Cluster\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      additionalPrinterColumns:\n        - jsonPath: .metadata.creationTimestamp\n          type: date\n          name: Age\n          description: The age of the report\n        - jsonPath: .status.summary.failCount\n          type: integer\n          name: Fail\n          priority: 1\n          description: The number of checks that failed with Danger status\n        - jsonPath: .status.summary.passCount\n          type: integer\n          name: Pass\n          priority: 1\n          description: The number of checks that passed\n      schema:\n        openAPIV3Schema:\n          type: object\n          required:\n            - apiVersion\n            - kind\n            - metadata\n            - spec\n          properties:\n            apiVersion:\n              type: string\n            kind:\n              type: string\n            metadata:\n              type: object\n            spec:\n              type: object\n              required:\n                - name\n                - description\n                - version\n                - cron\n                - controls\n              properties:\n                name:\n                  type: string\n                description:\n                  type: string\n                version:\n                  type: string\n                cron:\n                  type: string\n                  pattern: '^(((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1-5]{1}){1}([0-9]{1}){1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-9]{1}){1}|(([1]{1}){1}([0-9]{1}){1}){1}|([2]{1}){1}([0-3]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))) ((([\\*]{1}){1})|((\\*\\/){0,1}(([1-9]{1}){1}|(([1-2]{1}){1}([0-9]{1}){1}){1}|([3]{1}){1}([0-1]{1}){1}))|(jan|feb|mar|apr|may|jun|jul|aug|sep|okt|nov|dec)) ((([\\*]{1}){1})|((\\*\\/){0,1}(([0-7]{1}){1}))|(sun|mon|tue|wed|thu|fri|sat)))$'\n                  description: 'cron define the intervals for report generation'\n                controls:\n                  type: array\n                  items:\n                    type: object\n                    required:\n                      - name\n                      - id\n                      - kinds\n                      - mapping\n                      - severity\n                    properties:\n                      name:\n                        type: string\n                      description:\n                        type: string\n                      id:\n                        type: string\n                        description: 'id define the control check id'\n                      kinds:\n                        type: array\n                        items:\n                          type: string\n                          description: 'kinds define the list of kinds control check apply on , example: Node,Workload '\n                      mapping:\n                        type: object\n                        required:\n                          - scanner\n                          - checks\n                        properties:\n                          scanner:\n                            type: string\n                            pattern: '^config-audit$|^kube-bench$'\n                            description: 'scanner define the name of the scanner which produce data, currently only config-audit and kube-bench are supported'\n                          checks:\n                            type: array\n                            items:\n                              type: object\n                              required:\n                                - id\n                              properties:\n                                id:\n                                  type: string\n                                  description: 'id define the check id as produced by scanner'\n                      severity:\n                        type: string\n                        description: 'define the severity of the control'\n                        enum:\n                          - CRITICAL\n                          - HIGH\n                          - MEDIUM\n                          - LOW\n                          - UNKNOWN\n            status:\n              x-kubernetes-preserve-unknown-fields: true\n              type: object\n      subresources:\n        # status enables the status subresource.\n        status: { }\n  names:\n    singular: clustercompliancereport\n    plural: clustercompliancereports\n    kind: ClusterComplianceReport\n    listKind: ClusterComplianceReportList\n    categories: [ ]\n    shortNames:\n      - compliance\n</code></pre>","location":"design/design_compliance_report/#clustercompliancereport-crd"},{"title":"ClusterComplianceDetailReport CRD :","text":"<ul> <li>a new CRD <code>clustercompliancedetailreports.crd.yaml</code> will be added to include compliance detail check report</li> </ul> <pre><code>apiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: clustercompliancedetailreports.aquasecurity.github.io\n  labels:\n    app.kubernetes.io/managed-by: trivy-operator\n    app.kubernetes.io/version: \"0.14.1\"\nspec:\n  group: aquasecurity.github.io\n  versions:\n    - name: v1alpha1\n      served: true\n      storage: true\n      additionalPrinterColumns:\n        - jsonPath: .metadata.creationTimestamp\n          type: date\n          name: Age\n          description: The age of the report\n        - jsonPath: .report.summary.failCount\n          type: integer\n          name: Fail\n          priority: 1\n          description: The number of checks that failed with Danger status\n        - jsonPath: .report.summary.passCount\n          type: integer\n          name: Pass\n          priority: 1\n          description: The number of checks that passed\n      schema:\n        openAPIV3Schema:\n          x-kubernetes-preserve-unknown-fields: true\n          type: object\n  scope: Cluster\n  names:\n    singular: clustercompliancedetailreport\n    plural: clustercompliancedetailreports\n    kind: ClusterComplianceDetailReport\n    listKind: ClusterComplianceDetailReportList\n    categories: []\n    shortNames:\n      - compliancedetail \n</code></pre>","location":"design/design_compliance_report/#clustercompliancedetailreport-crd"},{"title":"Permission changes:","text":"<p>it is required to update <code>02-trivy-operator.rbac.yaml</code> rules to include new permissions to support the following tracked resources kind by NSA plugin with (get,list and watch):</p> <p>```yaml - apiGroups: [\"networking.k8s.io\"]   resources:     - networkpolicies   verbs:     - get     - list     - watch <pre><code>```yaml\n- apiGroups:\n      - \"\"\n    resources:\n      - resourcequota\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre></p>","location":"design/design_compliance_report/#permission-changes"},{"title":"NSA Tool Analysis","text":"Test Description Kind Tool Test     Non-root containers Check that container is not running as root Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield : kubernetes/policies/pss/restricted/3_runs_as_root.rego   Immutable container file systems check that container root file system is immutable Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/general/file_system_not_read_only.rego   Scan container images vulnerabilities scan container for vulnerabilities and misconfiguration Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Trivy Trivy   Privileged container Controls whether Pods can run privileged containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/2_privileged.rego   hostIPC Controls whether containers can share host process namespaces Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_ipc.rego   hostPID Controls whether containers can share host process namespaces. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_pid.rego   hostNetwork Controls whether containers can use the host network. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/1_host_network.rego   allowedHostPaths Limits containers to specific paths of the host file system. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest Need to be added to appshield : https://kubernetes.io/docs/concepts/policy/pod-security-policy/#volumes-and-file-systems   runAsUser , runAsGroup and supplementalGroups Controls whether container applications can run with root privileges or with root group membership Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/4_runs_with_a_root_gid.rego   allowPrivilegeEscalation Restricts escalation to root privileges. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/2_can_elevate_its_own_privileges.rego   seLinux Sets the SELinux context of the container. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/7_selinux_custom_options_set.rego   AppArmor annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/baseline/6_apparmor_policy_disabled.rego   seccomp annotations Sets the seccomp profile used to sandbox containers. Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/pss/restricted/5_runtime_default_seccomp_profile_not_set.rego   Protecting Pod service account tokens disable secret token been mount ,automountServiceAccountToken: false Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protecting_pod_service_account_tokens.rego   kube-system or kube-public namespace kube-system should should not be used by users Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/protect_core_components_namespace.rego   Use CNI plugin that supports NetworkPolicy API check cni plugin installed Node Kube-bench 5.3.1 Ensure that the CNI in use supports Network Policies (need to be fixed)   Create policies that select Pods using podSelector and/or the namespaceSelector Create policies that select Pods using podSelector and/or the namespaceSelector Pod,ReplicationController,ReplicaSet,StatefulSet,DaemonSet,Job,CronJob Conftest appshield: kubernetes/policies/advance/selector_usage_in_network_policies.rego   use a default policy to deny all ingress and egress traffic check that network policy deny all exist NetworkPolicy Kube-bench Add logic to kube-bench https://kubernetes.io/docs/concepts/services-networking/network-policies/   Use LimitRange and ResourceQuota policies to limit resources on a namespace or Pod level check the resource quota resource has been define ResourceQuota Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/concepts/policy/limit-range/   TLS encryption control plan disable insecure port Node Kube-bench 1.2.19 Ensure that the --insecure-port argument is set to 0   Etcd encryption encrypt etcd communication Node Kube-bench 2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate   Kubeconfig files ensure file permission Node Kube-bench 4.1.3, 4.1.4   Worker node segmentation node segmentation Node Kube-bench Note sure can be tested   Encryption check that encryption resource has been set EncryptionConfiguration Kube-bench Add Logic to kube-bench https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/   Encryption / secrets check encryption provider Node Kube-bench 1.2.3 Ensure that the --encryption-provider-config argument is set as   authentication make sure anonymous-auth is unset Node Kube-bench 1.2.1 Ensure that the --anonymous-auth argument is set to false   Role-based access control make sure -authorization-mode=RBAC Node Kube-bench 1.2.7/1.2.8 Ensure that the --authorization-mode argument is not set to AlwaysAllow   Audit policy file check that policy is configure Node Kube-bench 3.2.1 Ensure that a minimal audit policy is created   Audit log path check that log path is configure Node Kube-bench 1.2.22 Ensure that the --audit-log-path argument is set   Audit log max age check audit log aging Node Kube-bench 1.2.23 Ensure that the --audit-log-maxage argument is set to 30 or as appropriate   service mesh usage check service mesh is used in cluster Node Kube-bench Add Logic to kube-bench check service mesh existenace","location":"design/design_compliance_report/#nsa-tool-analysis"},{"title":"Open Items","text":"<ul> <li>compliance support for CLI</li> </ul>","location":"design/design_compliance_report/#open-items"},{"title":"Scan Container Images with Trivy Filesystem Scanner","text":"<p>Authors: Devendra Turkar, Daniel Pacak</p>","location":"design/design_trivy_file_system_scanner/"},{"title":"Overview","text":"<p>Trivy-Operator currently uses Trivy in [Standalone] or [ClientServer] mode to scan and generate VulnerabilityReports for container images by pulling the images from remote registries. Trivy-Operator scans a specified K8s workload by running the Trivy executable as a K8s Job. This approach implies that Trivy does not have access to images cached by the container runtime on cluster nodes. Therefore, to scan images from private registries Trivy-Operator reads ImagePullSecrets specified on workloads or on service accounts used by the workloads, and passes them down to Trivy executable as <code>TRIVY_USERNAME</code> and <code>TRIVY_PASSWORD</code> environment variables.</p> <p>Since ImagePullSecrets are not the only way to provide registry credential, the following alternatives are not currently supported by Trivy-Operator: 1. Pre-pulled images 2. Configuring nodes to authenticate to a private registry 3. Vendor-specific or local extension. For example, methods described on AWS ECR Private registry authentication.</p> <p>Even though we could resolve some of above-mentioned limitations with hostPath volume mounts to the container runtime socket, it would have its own disadvantages that we are trying to avoid. For example, more permissions to schedule scan Jobs and additional information about cluster's infrastructure such as location of the container runtime socket. </p>","location":"design/design_trivy_file_system_scanner/#overview"},{"title":"Solution","text":"","location":"design/design_trivy_file_system_scanner/#solution"},{"title":"TL;DR;","text":"<p>Use Trivy filesystem scanning to scan container images. The main idea, which is discussed in this proposal, is to schedule a scan Job on the same cluster node where the scanned workload. This allows Trivy to scan a filesystem of the container image which is already cached on that node without pulling the image from a remote registry. What's more, Trivy will scan container images from private registries without providing registry credentials (as ImagePullSecret or in any other proprietary way).</p>","location":"design/design_trivy_file_system_scanner/#tldr"},{"title":"Deep Dive","text":"<p>To scan a container image of a given K8s workload Trivy-Operator will create a corresponding container of a scan Job and override its entrypoint to invoke Trivy filesystem scanner.</p> <p>This approach requires Trivy executable to be downloaded and made available to the entrypoint. We'll do that by adding the init container to the scan Job. Such init container will use the Trivy container image to copy Trivy executable out to the emptyDir volume, which will be shared with the other containers.</p> <p>Another init container is required to download Trivy vulnerability database and save it to the mounted shared volume.</p> <p>Finally, the scan container will use shared volume with the Trivy executable and Trivy database to perform the actual filesystem scan. (See the provided Example to have a better idea of all containers defined by a scan Job and how they share data via the emptyDir volume.)</p>  <p>Note that the second init container is required in [Standalone] mode, which is the only mode supported by Trivy filesystem scanner at the time of writing this proposal.</p>  <p>We further restrict scan Jobs to run on the same node where scanned Pod is running and never pull images from remote registries by setting the <code>ImagePullPolicy</code> to <code>Never</code>. To determine the node for a scan Job Trivy-Operator will list active Pods controlled by the scanned workload. If the list is not empty it will take the node name from the first Pod, otherwise it will ignore the workload.</p>","location":"design/design_trivy_file_system_scanner/#deep-dive"},{"title":"Example","text":"<p>Let's assume that there's the <code>nginx</code> Deployment in the <code>poc-ns</code> namespace. It runs the <code>example.registry.com/nginx:1.16</code> container image from a private registry <code>example.registry.com</code>. Registry credentials are stored in the <code>private-registry</code> ImagePullSecret. (Alternatively, ImagePullSecret can be attached to a service account referred to by the Deployment.)</p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      imagePullSecrets:\n        - name: private-registry\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n</code></pre> <p>To scan the <code>nginx</code> container of the <code>nginx</code> Deployment, Trivy-Operator will create the following scan Job in the <code>trivy-system</code> namespace and observe it until it's Completed or Failed.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: trivy-system\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      restartPolicy: Never\n      # Explicit nodeName indicates our intention to schedule a scan pod\n      # on the same cluster node where the nginx workload is running.\n      # This could also imply considering taints and tolerations and other\n      # properties respected by K8s scheduler.\n      nodeName: kind-control-plane\n      volumes:\n        - name: scan-volume\n          emptyDir: { }\n      initContainers:\n        # The trivy-get-binary init container is used to copy out the trivy executable\n        # binary from the upstream Trivy container image, i.e. aquasec/trivy:0.19.2,\n        # to a shared emptyDir volume.\n        - name: trivy-get-binary\n          image: aquasec/trivy:0.19.2\n          command:\n            - cp\n            - -v\n            - /usr/local/bin/trivy\n            - /var/trivy-operator/trivy\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n        # The trivy-download-db container is using trivy executable binary\n        # from the previous step to download Trivy vulnerability database\n        # from GitHub releases page.\n        # This won't be required once Trivy supports ClientServer mode\n        # for the fs subcommand.\n        - name: trivy-download-db\n          image: aquasec/trivy:0.19.2\n          command:\n            - /var/trivy-operator/trivy\n            - --download-db-only\n            - --cache-dir\n            - /var/trivy-operator/trivy-db\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n      containers:\n        # The nginx container is based on the container image that\n        # we want to scan with Trivy. However, it has overwritten command (entrypoint)\n        # to invoke trivy file system scan. The scan results are output to stdout\n        # in JSON format, so we can parse them and store as VulnerabilityReport.\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n          # To scan image layers cached on a cluster node without pulling\n          # it from a remote registry.\n          imagePullPolicy: Never\n          securityContext:\n            # Trivy must run as root, so we set UID here.\n            runAsUser: 0\n          command:\n            - /var/trivy-operator/trivy\n            - --cache-dir\n            - /var/trivy-operator/trivy-db\n            - fs\n            - --format\n            - json\n            - /\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n</code></pre> <p>Notice that the scan Job does not use registry credentials stored in the <code>private-registry</code> ImagePullSecret at all. Also, the <code>ImagePullPolicy</code> for the <code>nginx</code> container is set to <code>Never</code> to avoid pulling the image from the <code>example.registry.com/nginx</code> repository that requires authentication. And finally, the <code>nodeName</code> property is explicitly set to <code>kube-control-plane</code> to make sure that the scan Job is scheduled on the same node as a Pod controlled by the <code>nginx</code> Deployment. (We assumed that there was at least one Pod controlled by the <code>nginx</code> Deployment, and it was scheduled on the <code>kube-control-plane</code> node.)</p> <p>Trivy must run as root so the scan Job defined the <code>securityContext</code> with the <code>runAsUser</code> property set to <code>0</code> UID.</p>","location":"design/design_trivy_file_system_scanner/#example"},{"title":"Remarks","text":"<ol> <li>The proposed solution won't work with the AlwaysPullImages admission controller, which might be enabled in    a multitenant cluster so that users can be assured that their private images can only be used by those who    have the credentials to pull them. (Thanks kfox1111 for pointing this out!)</li> <li>We cannot scan K8s workloads scaled down to 0 replicas because we cannot infer on which cluster node a scan Job should    run. (In general, a node name is only set on a running Pod.) But once a workload is scaled up, Trivy Operator    will receive the update event and will have another chance to scan it.</li> <li>It's hard to identify Pods managed by the CronJob controller, therefore we'll skip them.</li> <li>Trivy filesystem command does not work in [ClientServer] mode. Therefore, this solution is subject to the limits of    the [Standalone] mode. We plan to extend Trivy filesystem command to work in ClientServer mode and improve the    implementation of Trivy Operator once it's available.</li> <li>Trivy must run as root and this may be blocked by some Admission Controllers such as PodSecurityPolicy.</li> </ol>","location":"design/design_trivy_file_system_scanner/#remarks"},{"title":"Run Vulnerability Scan Job in Same namespace of workload","text":"","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/"},{"title":"Overview","text":"<p>When user runs a workload with private managed registry image(eg. image from ECR, ACR) and user is not using ImagePullSecret method to provide access to registry, then trivy operator has challenges to scan such workloads.  - Consider an example of ECR registry, there is one option available in which that user can associate IAM role to service account,  then workloads which are associated with this service account will get authorised to run with the image from that registry.   If user wants to get these images scanned using Trivy operator then currently we have only one way to do that.   User has to associate IAM role to trivy-operator service account, so with when scan job run with <code>trivy-operator</code>service   account, then Trivy will get appropriate permission to pull the image. To know more on how this mechanism works, please   refer to the documents ECR registry configuration, IAM role to service account, but, trivy cannot use permission   set on service account of workload.  </p> <p>Recently, there is one option added in Trivy plugin with Trivy fs command, In which Trivy scans the image which is  cached on a node. And to do that scan job is scheduled on same node where workload is running, so that Trivy can use a  cached image from a node. But, if we want to schedule these scan job on any node, then currently we dont have option to  do that, coz image might not be available on that node. Also, trivy cannot attach imagePullSecret available on the  workload pull the image. We also thought that when we have ImagePullSecret available on a workload, then we can use existing  option of Trivy image scan with which we can scan workload. To do that, trivy operator creates another secret  from existing ImagePullSecret so that registry credentials are provided to Trivy as Env var. But again,  we cannot reuse the same ImagePullSecret available on the workload.     </p>","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#overview"},{"title":"Solution","text":"<p>Consider there is an option given to enable running vulnerability scan jobs in the same namespace of workload. Operator detects it, so it can schedule and monitor scan jobs in same namespace where workload is running. And plugins will act  accordingly to utilize the service account and ImagePullSecret available on the workload.</p>","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#solution"},{"title":"Example","text":"","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example"},{"title":"Example 1","text":"<p>Consider trivy operator is running with Trivy image scan mode. And let's assume that there is an <code>nginx</code>  deployment in <code>poc-ns</code> namespace. It is running with image <code>12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16</code>.  This deployment is running with service account <code>poc-sa</code>, which is annotated with ARN: <code>arn:aws:iam::&lt;ACCOUNT_ID&gt;:role/IAM_ROLE_NAME</code></p> <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: v1\nautomountServiceAccountToken: true\nkind: ServiceAccount\nmetadata:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT_ID:role/IAM_ROLE_NAME\n  name: poc-sa\n  namespace: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: nginx\n  name: nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      serviceAccountName: poc-sa\n      containers:\n        - name: nginx\n          image: 12344534.dkr.ecr.us-west-2.amazonaws.com/amazon/nginx:1.16\n</code></pre>  <p>When a pod(<code>nginx-65b78bbbd4-nb5kl</code>) comes into running state from above deployment then pod will  have these env var to get access to ECR registry: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>, <code>AWS_WEB_IDENTITY_TOKEN_FILE</code> </p>  <p>To scan the <code>nginx</code> deployment, trivy-operator create following scan job in <code>poc-ns</code> namespace. And trivy-operator will monitor this job, and it will parse the result based on completion state of job. This job will run with same  service account(<code>poc-sa</code>) of workload.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: poc-ns\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      serviceAccountName: poc-sa\n      restartPolicy: Never\n      containers:\n      # containers from pod spec returned from existing Trivy plugin\n</code></pre>  <p>When a pod(<code>scan-vulnerabilityreport-ab3134-nfkst</code>) gets created from above job spec, then that pod will get injected  with these env var which will help scanner to get access to registry image: <code>AWS_REGION</code>, <code>AWS_ROLE_ARN</code>,  <code>AWS_WEB_IDENTITY_TOKEN_FILE</code></p>  <p>Pod will get injected with respective env vars to get access to registry image and Trivy scanner will use these credentials to pull an image for scanning.</p>","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example-1"},{"title":"Example 2","text":"<p>Consider another example, in which we want to perform vulnerability scan using Trivy <code>fs</code> command. Deployment <code>demo-nginx</code> is running in <code>poc-ns</code> namespace. This deployment is running with image  <code>example.registry.com/nginx:1.16</code> from private registry <code>example.registry.com</code>. Registry credentials are stored in  ImagePullSecret <code>private-registry</code>.  <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: poc-ns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-nginx\n  name: demo-nginx\n  namespace: poc-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      imagePullSecrets:\n        - name: private-registry\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n</code></pre></p> <p>To scan the <code>demo-nginx</code> deployment, trivy-operator create following scan job in <code>poc-ns</code> namespace. And trivy-operator  will monitor job, and it will parse the result based on completion state of job.</p> <pre><code>---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: scan-vulnerabilityreport-ab3134\n  namespace: poc-ns\nspec:\n  backoffLimit: 0\n  template:\n    spec:\n      # ImagePullSecret value will be copied from workload which we are scanning\n      imagePullSecrets:\n        - name: private-registry\n      restartPolicy: Never\n      volumes:\n        - name: scan-volume\n          emptyDir: { }\n      initContainers:\n        - name: trivy-get-binary\n          image: aquasec/trivy:0.19.2\n          command:\n            - cp\n            - -v\n            - /usr/local/bin/trivy\n            - /var/trivy-operator/trivy\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n        - name: trivy-download-db\n          image: aquasec/trivy:0.19.2\n          command:\n            - /var/trivy-operator/trivy\n            - --download-db-only\n            - --cache-dir\n            - /var/trivy-operator/trivy-db\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n      containers:\n        - name: nginx\n          image: example.registry.com/nginx:1.16\n          imagePullPolicy: IfNotPresent\n          securityContext:\n            # Trivy must run as root, so we set UID here.\n            runAsUser: 0\n          command:\n            - /var/trivy-operator/trivy\n            - --cache-dir\n            - /var/trivy-operator/trivy-db\n            - fs\n            - --format\n            - json\n            - /\n          volumeMounts:\n            - name: scan-volume\n              mountPath: /var/trivy-operator\n</code></pre> <p>If you observe in the job spec, this scan job will run in <code>poc-ns</code> namespace and it is running with image  <code>example.registry.com/nginx:1.16</code>. It is using ImagePullSecret <code>private-registry</code> which is available in same namespace.  With this approach trivy operator will not have to worry about managing(create/delete) of secret required for scanning. </p>","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#example-2"},{"title":"Notes","text":"<ol> <li>There are some points to consider before using this option<ul> <li>Scan jobs will run in different namespaces. This will create some activity in each namespace available in the cluster.  If we dont use this option then all scan jobs will only run in <code>trivy-operator</code> namespace, and user can see all  activity confined to single namespace i.e <code>trivy-operator</code>.</li> <li>As we will run scan job with service account of workload and if there are some very strict PSP defined in the cluster then scan job will be blocked due to the PSP.</li> </ul> </li> </ol>","location":"design/design_vuln_scan_job_in_same_namespace_of_workload/#notes"},{"title":"TTL scans","text":"","location":"design/ttl_scans/"},{"title":"Summary","text":"<p>Add an option to automatically delete old security reports. In this first version focus on vulnerability reports but in the long run we could add similar functionality to other reports as well.</p>","location":"design/ttl_scans/#summary"},{"title":"Motivation","text":"<p>In 537 we talk about a need to run nightly vulnerability scans of CVE:s. This way we can make sure to get new CVE reports for long time running pods as well.</p>","location":"design/ttl_scans/#motivation"},{"title":"Proposal","text":"<p>Add a environment variable to the operator, for example <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=86400</code>, this way we can add other ttl values for other reports as well. Adding this environment variable would add a annotation to the generated VulnerabilityReport that we can look for.</p> <p>Create a new controller that looks for changes in vulnerabilityreports and uses RequeueAfter calculated from the TTL annotation. At startup the operator will look through all existing vulnerabilityreports and delete existing ones where TTL have expired. If the TTL haven't expired the the vulnerabilityreports will be requeued and automatically checked again when the TTL have expired.</p> <p>We could calculate the ttl without having creating a new annotation to the reports but the verbosity of showing the users how long each report got a ttl outweighs the \"issue\" of generating a new annotation.</p>","location":"design/ttl_scans/#proposal"},{"title":"Example","text":"<p>Below you can see a shortened version of the yaml. Notice the <code>metadata.annotations.trivy-operator.aquasecurity.github.io/report-ttl</code> which is new. The operator would automatically apply the <code>trivy-operator.aquasecurity.github.io/report-ttl</code> annotation to all new reports that it generates assuming that the environment variable is set. In theory users could also extend the TTL manually for a specific report by changing the trivy-operator.aquasecurity.github.io/report-ttl annotation per VulnerabilityReport.</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: VulnerabilityReport\nmetadata:\n  creationTimestamp: \"2021-12-08T12:03:48Z\"\n  annotations:\n    trivy-operator.aquasecurity.github.io/report-ttl: 24h\n  labels:\n    resource-spec-hash: 86b58dcb99\n    trivy-operator.container.name: manager\n    trivy-operator.resource.kind: ReplicaSet\n    trivy-operator.resource.name: source-controller-b5d5cfdf4\n    trivy-operator.resource.namespace: flux-system\n  name: replicaset-source-controller-b5d5cfdf4-manager\nreport:\n  artifact:\n    repository: fluxcd/source-controller\n    tag: v0.16.1\n  registry:\n    server: ghcr.io\n  scanner:\n    name: Trivy\n    vendor: Aqua Security\n    version: 0.19.2\n  summary:\n    criticalCount: 0\n    highCount: 0\n    lowCount: 0\n    mediumCount: 0\n    unknownCount: 0\n  updateTimestamp: \"2021-12-08T12:03:48Z\"\n  vulnerabilities: []\n</code></pre> <p>Another option is to define a new report entry, the positive thing with that is that we can define the input type, in our case a <code>time.Duration</code>.</p>","location":"design/ttl_scans/#example"},{"title":"Alternatives","text":"<p>Another \"simpler\" option could be to add the same environment variable <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL=2h</code> but instead of using RequeueAfter in the controller we could create a cronjob/job that runs once an hour and check for the same annotation.</p> <p>The bad thing hear is that we would have to manage yet another cronjob/job. We would also have to mange a new binary feature flag to run in cronjob cleanup mode. It would also trigger removal of multiple reports at the same time, compared to the event driven solution that would be much more precise per report and thus spreading out the new reports more.</p> <p>But the good thing is that everyone knows how jobs/cronjobs works especially since it's already well used within the trivy-operator operator.</p>","location":"design/ttl_scans/#alternatives"},{"title":"Lens Extension","text":"<p>Lens provides the full situational awareness for everything that runs in Kubernetes. It's lowering the barrier of entry for people just getting started and radically improving productivity for people with more experience. Lens Extensions API is used to add custom visualizations and functionality to accelerate development workflows for all the technologies and services that integrate with Kubernetes.</p> <p>Trivy-Operator Lens Extension provides visibility into vulnerability assessment reports for Kubernetes workloads stored as custom resources.</p>","location":"integrations/lens/"},{"title":"Metrics","text":"<p><code>trivy-operator</code> exposed a <code>/metrics</code> endpoint by default  with metrics for vulnerabilities, exposed secrets, and configaudits.</p>","location":"integrations/metrics/"},{"title":"Webhook Integration","text":"<p>Trivy Operator allows you to send reports externally to a webhook as they get produced. This is useful in cases where you would like to \"set-and-forget\" the operator and monitor the reports elsewhere. It's also useful when you have to make decisions based on a report, e.g. prune a vulnerable image, remove a deployment with exposed secrets etc.</p> <p>The latter use case can be fulfilled by using a SOAR tool Postee. Out of the box, Postee offers a variety of integrations with other third party services such as ServiceNow, Slack, AWS Security Hub and many more.  </p> <p></p> <p>You can enable the Webhook integration as follows:</p> <ol> <li>Required: Set <code>OPERATOR_WEBHOOK_BROADCAST_URL</code> to the webhook endpoint you'd like to send the reports to.</li> <li>Optional: Set <code>OPERATOR_WEBHOOK_BROADCAST_TIMEOUT</code> to a time limit that suites your use case. Default is <code>30s</code>.</li> </ol> <p>The Webhook integration is only able to send <code>vulnerabilityreport</code> and <code>exposedsecretreport</code> type of reports.</p>","location":"integrations/webhook/"},{"title":"Trivy-Operator","text":"","location":"operator/"},{"title":"Overview","text":"<p>This operator automatically updates security report resources in response to workload and other changes on a Kubernetes cluster - for example, initiating a vulnerability scan and configuration audit when a new Pod is started.</p>   Workload reconcilers discover K8s controllers, manage scan jobs, and create VulnerabilityReport and ConfigAuditReport objects.  <p>In other words, the desired state for the controllers managed by this operator is that for each workload or node there are security reports stored in the cluster as custom resources. Each custom resource is owned by a built-in resource to inherit its life cycle. Beyond that, we take advantage of Kubernetes garbage collector to automatically delete stale reports and trigger rescan. For example, deleting a ReplicaSet will delete controlee VulnerabilityReports, whereas deleting a VulnerabilityReport owned by a ReplicaSet will rescan that ReplicaSet and eventually recreate the VulnerabilityReport.</p> <p>Rescan is also triggered whenever a config of a configuration audit plugin has changed. For example, when a new OPA policy script is added to the Confest plugin config. This is implemented by adding the label named <code>plugin-config-hash</code> to ConfigAuditReport instances. The plugins' config reconciler watches the ConfigMap that holds plugin settings and computes a hash from the ConfigMap's data. The hash is then compared with values of the <code>plugin-config-hash</code> labels. If hashes are not equal then affected ConfigAuditReport objects are deleted, which in turn triggers rescan - this time with new plugin's configuration.</p>   Plugin configuration reconciler deletes ConfigAuditReports whenever the configuration changes.   <p>Warning</p> <p>Currently, the operator supports vulnerabilityreports, configauditreports security resources.      We also plan to implement rescan on configurable schedule, for example every 24 hours.</p>","location":"operator/#overview"},{"title":"What's Next?","text":"<ul> <li>Install the operator and follow the Getting Started guide.</li> </ul>","location":"operator/#whats-next"},{"title":"Configuration","text":"<p>You can configure Trivy-Operator to control it's behavior and adapt it to your needs. Aspects of the operator machinery are configured using environment variables on the operator Pod, while aspects of the scanning behavior are controlled by ConfigMaps and Secrets.</p>","location":"operator/configuration/"},{"title":"Operator Configuration","text":"NAME DEFAULT DESCRIPTION     <code>OPERATOR_NAMESPACE</code> N/A See Install modes   <code>OPERATOR_TARGET_NAMESPACES</code> N/A See Install modes   <code>OPERATOR_EXCLUDE_NAMESPACES</code> N/A A comma separated list of namespaces (or glob patterns) to be excluded from scanning in all namespaces Install mode.   <code>OPERATOR_TARGET_WORKLOADS</code> All workload resources A comma seperated list of Kubernetes workloads to be included in the vulnerability and config-audit scans   <code>OPERATOR_SERVICE_ACCOUNT</code> <code>trivy-operator</code> The name of the service account assigned to the operator's pod   <code>OPERATOR_LOG_DEV_MODE</code> <code>false</code> The flag to use (or not use) development mode (more human-readable output, extra stack traces and logging information, etc).   <code>OPERATOR_SCAN_JOB_TIMEOUT</code> <code>5m</code> The length of time to wait before giving up on a scan job   <code>OPERATOR_CONCURRENT_SCAN_JOBS_LIMIT</code> <code>10</code> The maximum number of scan jobs create by the operator   <code>OPERATOR_SCAN_JOB_RETRY_AFTER</code> <code>30s</code> The duration to wait before retrying a failed scan job   <code>OPERATOR_BATCH_DELETE_LIMIT</code> <code>10</code> The maximum number of config audit reports deleted by the operator when the plugin's config has changed.   <code>OPERATOR_BATCH_DELETE_DELAY</code> <code>10s</code> The duration to wait before deleting another batch of config audit reports.   <code>OPERATOR_METRICS_BIND_ADDRESS</code> <code>:8080</code> The TCP address to bind to for serving Prometheus metrics. It can be set to <code>0</code> to disable the metrics serving.   <code>OPERATOR_HEALTH_PROBE_BIND_ADDRESS</code> <code>:9090</code> The TCP address to bind to for serving health probes, i.e. <code>/healthz/</code> and <code>/readyz/</code> endpoints.   <code>OPERATOR_VULNERABILITY_SCANNER_ENABLED</code> <code>true</code> The flag to enable vulnerability scanner   <code>OPERATOR_CONFIG_AUDIT_SCANNER_ENABLED</code> <code>false</code> The flag to enable configuration audit scanner   <code>OPERATOR_RBAC_ASSESSMENT_SCANNER_ENABLED</code> <code>true</code> The flag to enable rbac assessment scanner   <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>true</code> The flag to enable config audit scanner to only scan the current revision of a deployment   <code>OPERATOR_CONFIG_AUDIT_SCANNER_BUILTIN</code> <code>true</code> The flag to enable built-in configuration audit scanner   <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> <code>true</code> The flag to enable vulnerability scanner to only scan the current revision of a deployment   <code>OPERATOR_ACCESS_GLOBAL_SECRETS_SERVICE_ACCOUNTS</code> <code>true</code> The flag to enable access to global secrets/service accounts to allow <code>vulnerability scan job</code> to pull images from private registries   <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL</code> <code>\"24h\"</code> The flag to set how long a vulnerability report should exist. When a old report is deleted a new one will be created by the controller. It can be set to <code>\"\"</code> to disabled the TTL for vulnerability scanner.   <code>OPERATOR_LEADER_ELECTION_ENABLED</code> <code>false</code> The flag to enable operator replica leader election   <code>OPERATOR_LEADER_ELECTION_ID</code> <code>trivy-operator-lock</code> The name of the resource lock for leader election   <code>OPERATOR_EXPOSED_SECRET_SCANNER_ENABLED</code> <code>true</code> The flag to enable exposed secret scanner   <code>OPERATOR_WEBHOOK_BROADCAST_URL</code> <code>\"\"</code> The flag to enable operator reports to be sent to a webhook endpoint. \"\" means that this feature is disabled   <code>OPERATOR_WEBHOOK_BROADCAST_TIMEOUT</code> <code>30s</code> The flag to set operator webhook timeouts, if webhook broadcast is enabled   <code>OPERATOR_PRIVATE_REGISTRY_SCAN_SECRETS_NAMES</code> <code>{}</code> The flag to provide information about names of the secrets for different namespaces to use them for authentication in private registries if there are no imagePullSecrets in Service Accounts and/or in Pod's Spec    <p>The values of the <code>OPERATOR_NAMESPACE</code> and <code>OPERATOR_TARGET_NAMESPACES</code> determine the install mode, which in turn determines the multitenancy support of the operator.</p>    MODE OPERATOR_NAMESPACE OPERATOR_TARGET_NAMESPACES DESCRIPTION     OwnNamespace <code>operators</code> <code>operators</code> The operator can be configured to watch events in the namespace it is deployed in.   SingleNamespace <code>operators</code> <code>foo</code> The operator can be configured to watch for events in a single namespace that the operator is not deployed in.   MultiNamespace <code>operators</code> <code>foo,bar,baz</code> The operator can be configured to watch for events in more than one namespace.   AllNamespaces <code>operators</code> (blank string) The operator can be configured to watch for events in all namespaces.","location":"operator/configuration/#operator-configuration"},{"title":"Example - configure namespaces to scan","text":"<p>To change the target namespace from all namespaces to the <code>default</code> namespace edit the <code>trivy-operator</code> Deployment and change the value of the <code>OPERATOR_TARGET_NAMESPACES</code> environment variable from the blank string (<code>\"\"</code>) to the <code>default</code> value.</p>","location":"operator/configuration/#example-configure-namespaces-to-scan"},{"title":"Scanning configuration","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>vulnerabilityReports.scanner</code> <code>Trivy</code> The name of the plugin that generates vulnerability reports. Either <code>Trivy</code> or <code>Aqua</code>.   <code>vulnerabilityReports.scanJobsInSameNamespace</code> <code>\"false\"</code> Whether to run vulnerability scan jobs in same namespace of workload. Set <code>\"true\"</code> to enable.   <code>scanJob.tolerations</code> N/A JSON representation of the tolerations to be applied to the scanner pods so that they can run on nodes with matching taints. Example: <code>'[{\"key\":\"key1\", \"operator\":\"Equal\", \"value\":\"value1\", \"effect\":\"NoSchedule\"}]'</code>   <code>scanJob.nodeSelector</code> N/A JSON representation of the [nodeSelector] to be applied to the scanner pods so that they can run on nodes with matching labels. Example: <code>'{\"example.com/node-type\":\"worker\", \"cpu-type\": \"sandylake\"}'</code>   <code>scanJob.annotations</code> N/A One-line comma-separated representation of the annotations which the user wants the scanner pods to be annotated with. Example: <code>foo=bar,env=stage</code> will annotate the scanner pods with the annotations <code>foo: bar</code> and <code>env: stage</code>   <code>scanJob.templateLabel</code> N/A One-line comma-separated representation of the template labels which the user wants the scanner pods to be labeled with. Example: <code>foo=bar,env=stage</code> will labeled the scanner pods with the labels <code>foo: bar</code> and <code>env: stage</code>   <code>scanJob.podTemplatePodSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner pods to be secured with. Example: <code>{\"RunAsUser\": 1000, \"RunAsGroup\": 1000, \"RunAsNonRoot\": true}</code>   <code>scanJob.podTemplateContainerSecurityContext</code> N/A One-line JSON representation of the template securityContext which the user wants the scanner containers (and their initContainers) to be amended with. Example: <code>{\"allowPrivilegeEscalation\": false, \"capabilities\": { \"drop\": [\"ALL\"]},\"privileged\": false, \"readOnlyRootFilesystem\": true }</code>","location":"operator/configuration/#scanning-configuration"},{"title":"Example - patch ConfigMap","text":"<p>By default Trivy displays vulnerabilities with all severity levels (<code>UNKNOWN</code>, <code>LOW</code>, <code>MEDIUM</code>, <code>HIGH</code>, <code>CRITICAL</code>). To display only <code>HIGH</code> and <code>CRITICAL</code> vulnerabilities by patching the <code>trivy.severity</code> value in the <code>trivy-operator-trivy-config</code> ConfigMap:</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.severity\": \"HIGH,CRITICAL\"\n  }\n}\nEOF\n)\"\n</code></pre>","location":"operator/configuration/#example-patch-configmap"},{"title":"Example - patch Secret","text":"<p>To set the GitHub token used by Trivy scanner add the <code>trivy.githubToken</code> value to the <code>trivy-operator-trivy-config</code> Secret:</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n &lt;your token&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>","location":"operator/configuration/#example-patch-secret"},{"title":"Example - delete a key","text":"<p>The following <code>kubectl patch</code> command deletes the <code>trivy.httpProxy</code> key:</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n  --type json \\\n  -p '[{\"op\": \"remove\", \"path\": \"/data/trivy.httpProxy\"}]'\n</code></pre>","location":"operator/configuration/#example-delete-a-key"},{"title":"Quick Start","text":"","location":"operator/quick-start/"},{"title":"Before you Begin","text":"<p>You need to have a Kubernetes cluster, and the kubectl command-line tool must be configured to communicate with your cluster. If you do not already have a cluster, you can create one by installing minikube, kind or microk8s, or you can use the following Kubernetes playground.</p> <p>You also need the Trivy-Operator to be installed in the <code>trivy-system</code> namespace, e.g. with kubectl or Helm. Let's also assume that the operator is configured to discover built-in Kubernetes resources in all namespaces, except <code>kube-system</code> and <code>trivy-system</code>.</p>","location":"operator/quick-start/#before-you-begin"},{"title":"Workloads Scanning","text":"<p>Let's create the <code>nginx</code> Deployment that we know is vulnerable:</p> <pre><code>kubectl create deployment nginx --image nginx:1.16\n</code></pre> <p>When the <code>nginx</code> Deployment is created, the operator immediately detects its current revision (aka active ReplicaSet) and scans the <code>nginx:1.16</code> image for vulnerabilities. It also audits the ReplicaSet's specification for common pitfalls such as running the <code>nginx</code> container as root.</p> <p>If everything goes fine, the operator saves scan reports as VulnerabilityReport and ConfigAuditReport resources in the <code>default</code> namespace. Reports are named after the scanned ReplicaSet. For image vulnerability scans, the operator creates a VulnerabilityReport for each different container. In this example there is just one container image called <code>nginx</code>:</p> <pre><code>kubectl get vulnerabilityreports -o wide\n</code></pre>  Result <pre><code>NAME                                REPOSITORY      TAG    SCANNER   AGE   CRITICAL   HIGH   MEDIUM   LOW   UNKNOWN\nreplicaset-nginx-78449c65d4-nginx   library/nginx   1.16   Trivy     85s   33         62     49       114   1\n</code></pre>  <pre><code>kubectl get configauditreports -o wide\n</code></pre>  Result <pre><code>NAME                          SCANNER     AGE    CRITICAL  HIGH   MEDIUM   LOW\nreplicaset-nginx-78449c65d4   Trivy       2m7s      0         0      6        7\n</code></pre>  <p>Notice that scan reports generated by the operator are controlled by Kubernetes workloads. In our example, VulnerabilityReport and ConfigAuditReport resources are controlled by the active ReplicaSet of the <code>nginx</code> Deployment:</p> <pre><code>kubectl tree deploy nginx\n</code></pre>  Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h2m\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h2m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              2m31s\ndefault      \u251c\u2500Pod/nginx-78449c65d4-5wvdx                             True           7h2m\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              2m7s\n</code></pre>   <p>Note</p>  <p>The tree command is a kubectl plugin to browse Kubernetes object hierarchies as a tree.</p> <p>Moving forward, let's update the container image of the <code>nginx</code> Deployment from <code>nginx:1.16</code> to <code>nginx:1.17</code>. This will trigger a rolling update of the Deployment and eventually create another ReplicaSet.</p> <pre><code>kubectl set image deployment nginx nginx=nginx:1.17\n</code></pre> <p>Even this time the operator will pick up changes and rescan our Deployment with updated configuration:</p> <pre><code>kubectl tree deploy nginx\n</code></pre>  Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h5m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              2m36s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              2m36s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           2m36s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              2m22s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h5m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              5m46s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              5m22s\n</code></pre>  <p>By following this guide you could realize that the operator knows how to attach VulnerabilityReport and ConfigAuditReport resources to build-in Kubernetes objects. What's more, in this approach where a custom resource inherits a life cycle of the built-in resource we could leverage Kubernetes garbage collection. For example, when the previous ReplicaSet named <code>nginx-78449c65d4</code> is deleted the VulnerabilityReport named <code>replicaset-nginx-78449c65d4-nginx</code> as well as the ConfigAuditReport named <code>replicaset-nginx-78449c65d46</code> are automatically garbage collected.</p>  <p>Tip</p>  <p>If you do not want only the latest ReplicaSet in your Deployment to be scanned for vulnerabilities, you can set the value of the <code>OPERATOR_VULNERABILITY_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>false</code> in the operator's deployment descriptor.</p>  <p>Tip</p>  <p>If you do not want only the latest ReplicaSet in your Deployment to be scanned for config audit, you can set the value of the <code>OPERATOR_CONFIG_AUDIT_SCANNER_SCAN_ONLY_CURRENT_REVISIONS</code> environment variable to <code>false</code> in the operator's deployment descriptor.</p>  <p>Tip</p>  <p>You can get and describe <code>vulnerabilityreports</code> and <code>configauditreports</code> as built-in Kubernetes objects: <pre><code>kubectl get vulnerabilityreport replicaset-nginx-5fbc65fff-nginx -o json\nkubectl describe configauditreport replicaset-nginx-5fbc65fff\n</code></pre></p> <p>Notice that scaling up the <code>nginx</code> Deployment will not schedule new scans because all replica Pods refer to the same Pod template defined by the <code>nginx-5fbc65fff</code> ReplicaSet.</p> <pre><code>kubectl scale deploy nginx --replicas 3\n</code></pre> <pre><code>kubectl tree deploy nginx\n</code></pre>  Result <pre><code>NAMESPACE  NAME                                                       READY  REASON  AGE\ndefault    Deployment/nginx                                           -              7h6m\ndefault    \u251c\u2500ReplicaSet/nginx-5fbc65fff                               -              4m7s\ndefault    \u2502 \u251c\u2500ConfigAuditReport/replicaset-nginx-5fbc65fff           -              4m7s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-458n7                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-fk847                              True           8s\ndefault    \u2502 \u251c\u2500Pod/nginx-5fbc65fff-j7zl2                              True           4m7s\ndefault    \u2502 \u2514\u2500VulnerabilityReport/replicaset-nginx-5fbc65fff-nginx   -              3m53s\ndefault    \u2514\u2500ReplicaSet/nginx-78449c65d4                              -              7h6m\ndefault      \u251c\u2500ConfigAuditReport/replicaset-nginx-78449c65d4          -              7m17s\ndefault      \u2514\u2500VulnerabilityReport/replicaset-nginx-78449c65d4-nginx  -              6m53s\n</code></pre>  <p>Finally, when you delete the <code>nginx</code> Deployment, orphaned security reports will be deleted in the background by the Kubernetes garbage collection controller.</p> <pre><code>kubectl delete deploy nginx\n</code></pre> <pre><code>kubectl get vuln,configaudit\n</code></pre>  Result <pre><code>No resources found in default namespace.\n</code></pre>   <p>Tip</p>  <p>Use <code>vuln</code> and <code>configaudit</code> as short names for <code>vulnerabilityreports</code> and <code>configauditreports</code> resources.</p>  <p>Note</p>  <p>The validity period for VulnerabilityReports by setting the duration as the value of the <code>OPERATOR_VULNERABILITY_SCANNER_REPORT_TTL</code> environment variable. The value is set to <code>24h</code> by default.</p> <p>The reports wil be deleted after 24 hours. When a VulnerabilityReport gets deleted Trivy-Operator will automatically rescan the resource.</p>","location":"operator/quick-start/#workloads-scanning"},{"title":"What's Next?","text":"<ul> <li>Find out how the operator scans workloads that use container images from [Private Registries].</li> <li>By default, the operator uses Trivy as [Vulnerability Scanner] and Polaris as [Configuration Checker], but you can   choose other tools that are integrated with Trivy-Operator or even implement you own plugin.</li> </ul>","location":"operator/quick-start/#whats-next"},{"title":"Troubleshooting the Trivy Operator","text":"<p>The Trivy Operator installs several Kubernetes resources into your Kubernetes cluster.</p> <p>Here are the common steps to check whether the operator is running correctly and to troubleshoot common issues.</p> <p>So in addition to this section, you might want to check issues, discussion forum, or Slack to see if someone from the community had similar problems before.</p> <p>Also note that Trivy Operator is based on existing Aqua OSS project - [Starboard], and shares some of the design, principles and code with it. Existing content that relates to Starboard Operator might also be relevant for Trivy Operator, and Starboard's issues, discussion forum, or Slack might also be interesting to check. In some cases you might want to refer to Starboard's Design documents</p>","location":"operator/troubleshooting/"},{"title":"Installation","text":"<p>Make sure that the latest version of the Trivy Operator is installed. For this, have a look at the installation options.</p> <p>For instance, if your are using the Helm deployment, you need to check the Helm Chart version deployed to your cluster. You can check the Helm Chart version with the following command: <pre><code>helm list -n trivy-system\n</code></pre></p>","location":"operator/troubleshooting/#installation"},{"title":"Operator Pod Not Running","text":"<p>The Trivy Operator will run a pod inside your cluster. If you have followed the installation guide, you will have installed the Operator to the <code>trivy-system</code>.</p> <p>Make sure that the pod is in the <code>Running</code> status: <pre><code>kubectl get pods -n trivy-system\n</code></pre></p> <p>This is how it will look if it is running okay:</p> <pre><code>NAMESPACE            NAME                                         READY   STATUS    RESTARTS      AGE\ntrivy-system     trivy-operator-6c9bd97d58-hsz4g          1/1     Running   5 (19m ago)   30h\n</code></pre> <p>If the pod is in <code>Failed</code>, <code>Pending</code>, or <code>Unknown</code> check the events and the logs of the pod.</p> <p>First, check the events, since they might be more descriptive of the problem. However, if the events do not give a clear reason why the pod cannot spin up, then you want to check the logs, which provide more detail.</p> <pre><code>kubectl describe pod &lt;POD-NAME&gt; -n trivy-system\n</code></pre> <p>To check the logs, use the following command: <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> <p>If your pod is not running, try to look for errors as they can give an indication on the problem.</p> <p>If there are too many logs messages, try deleting the Trivy pod and observe its behavior upon restarting. A new pod should spin up automatically after deleting the failed pod.</p>","location":"operator/troubleshooting/#operator-pod-not-running"},{"title":"ImagePullBackOff or ErrImagePull","text":"<p>Check the status of the Trivy Operator pod running inside of your Kubernetes cluster. If the Status is ImagePullBackOff or ErrImagePull, it means that the Operator either</p> <ul> <li>tries to access the wrong image</li> <li>cannot pull the image from the registry</li> </ul> <p>Make sure that you are providing the right resources upon installing the Trivy Operator.</p>","location":"operator/troubleshooting/#imagepullbackoff-or-errimagepull"},{"title":"CrashLoopBackOff","text":"<p>If your pod is in <code>CrashLoopBackOff</code>, it is likely the case that the pod cannot be scheduled on the Kubernetes node that it is trying to schedule on. In this case, you want to investigate further whether there is an issue with the node. It could for instance be the case that the node does not have sufficient resources.</p>","location":"operator/troubleshooting/#crashloopbackoff"},{"title":"Reconcilation Error","text":"<p>It could happen that the pod appears to be running normally but does not reconcile the resources inside of your Kubernetes cluster.</p> <p>Check the logs for reconcilation errors: <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> <p>If this is the case, the Trivy Operator likely does not have the right configurations to access your resource.</p>","location":"operator/troubleshooting/#reconcilation-error"},{"title":"Operator does not Create VulnerabilityReports","text":"<p>VulnerabilityReports are owned and controlled by the immediate Kubernetes workload. Every VulnerabilityReport of a pod is thus, linked to a ReplicaSet. In case the Trivy Operator does not create a VulnerabilityReport for your workloads, it could be that it is not monitoring the namespace that your workloads are running on.</p> <p>An easy way to check this is by looking for the <code>ClusterRoleBinding</code> for the Trivy Operator:</p> <pre><code>kubectl get ClusterRoleBinding | grep \"trivy-operator\"\n</code></pre> <p>Alternatively, you could use the <code>kubectl-who-can</code> plugin by Aqua:</p> <pre><code>$ kubectl who-can list vulnerabilityreports\nNo subjects found with permissions to list vulnerabilityreports assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                         TYPE            SA-NAMESPACE\ncluster-admin                                system:masters                  Group\ntrivy-operator                           trivy-operator              ServiceAccount  trivy-system\nsystem:controller:generic-garbage-collector  generic-garbage-collector       ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller            ServiceAccount  kube-system\nsystem:controller:resourcequota-controller   resourcequota-controller        ServiceAccount  kube-system\nsystem:kube-controller-manager               system:kube-controller-manager  User\n</code></pre> <p>If the <code>ClusterRoleBinding</code> does not exist, Trivy currently cannot monitor any namespace outside of the <code>trivy-system</code> namespace.</p> <p>For instance, if you are using the Helm Chart, you want to make sure to set the <code>targetNamespace</code> to the namespace that you want the Operator to monitor.</p> <p>The operator also could not be configured to scan the workload you are expecting. Check to make sure <code>OPERATOR_TARGET_WORKLOADS</code> is set correctly in your configuration. This allows you to specify which workload types to be scanned. </p> <p>For example, by default in the Helm Chart values, the following Kubernetes workloads are configured to be scanned <code>\"pod,replicaset,replicationcontroller,statefulset,daemonset,cronjob,job\"</code>.</p>","location":"operator/troubleshooting/#operator-does-not-create-vulnerabilityreports"},{"title":"Helm","text":"<p>Helm, which is de facto standard package manager for Kubernetes, allows installing applications from parameterized YAML manifests called Helm charts.</p> <p>To address shortcomings of static YAML manifests we provide the Helm chart to deploy the Trivy-Operator. The Helm chart supports all Install Modes.</p> <p>As an example, let's install the operator in the <code>trivy-system</code> namespace and configure it to select all namespaces, except <code>kube-system</code> and <code>trivy-system</code>:</p> <ol> <li>Clone the chart directory:    <pre><code>git clone --depth 1 --branch v0.4.0 https://github.com/aquasecurity/trivy-operator.git\ncd trivy-operator\n</code></pre>    Or add Aqua chart repository:    <pre><code>helm repo add aqua https://aquasecurity.github.io/helm-charts/\nhelm repo update\n</code></pre></li> <li>Install the chart from a local directory:    <pre><code>helm install trivy-operator ./deploy/helm \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --set=\"trivy.ignoreUnfixed=true\"\n</code></pre>    Or install the chart from the Aqua chart repository:    <pre><code>helm install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --set=\"trivy.ignoreUnfixed=true\" \\\n  --version 0.4.0\n</code></pre>    There are many values in the chart that can be set to configure Trivy-Operator.</li> <li>Check that the <code>trivy-operator</code> Helm release is created in the <code>trivy-system</code> namespace, and it has status    <code>deployed</code>:    <pre><code>$ helm list -n trivy-system\nNAME                 NAMESPACE           REVISION    UPDATED                                 STATUS      CHART                       APP VERSION\ntrivy-operator   trivy-system    1           2021-01-27 20:09:53.158961 +0100 CET    deployed    trivy-operator-0.4.0    0.4.0\n</code></pre>    To confirm that the operator is running, check that the <code>trivy-operator</code> Deployment in the <code>trivy-system</code>    namespace is available and all its containers are ready:    <pre><code>$ kubectl get deployment -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></li> </ol>","location":"operator/installation/helm/"},{"title":"Install as Helm dependency","text":"<p>There are cases, when potential chart developers want to add the operator as dependency. An example would be the creation of an umbrella chart for an application, which depends on 3d-party charts.</p> <p>In this case, It maybe not suitable to install the Trivy Operator in the same namespace as the main application. Instead, we can use the Helm value <code>operator.namespace</code> to define a namespace where only the operator will be installed. The Operator chart will then either create a new namespace if not existing or use the existing one.</p>","location":"operator/installation/helm/#install-as-helm-dependency"},{"title":"Uninstall","text":"<p>You can uninstall the operator with the following command:</p> <pre><code>helm uninstall trivy-operator -n trivy-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the <code>helm install</code> command:</p>  <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd clustercompliancereports.aquasecurity.github.io\nkubectl delete crd clustercompliancedetailreports.aquasecurity.github.io\n</code></pre>","location":"operator/installation/helm/#uninstall"},{"title":"kubectl","text":"<p>Kubernetes Yaml deployment files are available on GitHub in https://github.com/aquasecurity/trivy-operator under <code>/deploy/static</code>.</p>","location":"operator/installation/kubectl/"},{"title":"Example - Deploy from GitHub","text":"<p>This will install the operator in the <code>trivy-system</code> namespace and configure it to scan all namespaces, except <code>kube-system</code> and <code>trivy-system</code>:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/v0.4.0/deploy/static/trivy-operator.yaml\n</code></pre> <p>To confirm that the operator is running, check that the <code>trivy-operator</code> Deployment in the <code>trivy-system</code> namespace is available and all its containers are ready:</p> <pre><code>$ kubectl get deployment -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre> <p>If for some reason it's not ready yet, check the logs of the <code>trivy-operator</code> Deployment for errors:</p> <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre>","location":"operator/installation/kubectl/#example-deploy-from-github"},{"title":"Advanced Configuration","text":"<p>You can configure Trivy-Operator to control it's behavior and adapt it to your needs. Aspects of the operator machinery are configured using environment variables on the operator Pod, while aspects of the scanning behavior are controlled by ConfigMaps and Secrets. To learn more, please refer to the Configuration documentation.</p>","location":"operator/installation/kubectl/#advanced-configuration"},{"title":"Uninstall","text":"<p>Danger</p> <p>Uninstalling the operator and deleting custom resource definitions will also delete all generated security reports.</p>  <p>You can uninstall the operator with the following command:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/aquasecurity/trivy-operator/v0.4.0/deploy/static/trivy-operator.yaml\n</code></pre>","location":"operator/installation/kubectl/#uninstall"},{"title":"Operator Lifecycle Manager","text":"<p>The Operator Lifecycle Manager (OLM) provides a declarative way to install and upgrade operators and their dependencies.</p> <p>You can install the Trivy operator from OperatorHub.io or ArtifactHUB by creating the OperatorGroup, which defines the operator's multitenancy, and Subscription that links everything together to run the operator's pod.</p> <p>As an example, let's install the operator from the OperatorHub catalog in the <code>trivy-system</code> namespace and configure it to watch the <code>default</code> namespaces:</p> <ol> <li> <p>Install the Operator Lifecycle Manager:    <pre><code>curl -L https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.20.0/install.sh -o install.sh\nchmod +x install.sh\n./install.sh v0.20.0\n</code></pre></p> </li> <li> <p>Create the namespace to install the operator in:    <pre><code>kubectl create ns trivy-system\n</code></pre></p> </li> <li>Create the OperatorGroup to select all namespaces:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: trivy-operator-group\n  namespace: trivy-system\nEOF\n</code></pre></li> <li> <p>Install the operator by creating the Subscription:    <pre><code>cat &lt;&lt; EOF | kubectl apply -f -\napiVersion: operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: trivy-operator-subscription\n  namespace: trivy-system\nspec:\n  channel: alpha\n  name: trivy-operator\n  source: operatorhubio-catalog\n  sourceNamespace: olm\n  installPlanApproval: Automatic\n  config:\n    env:\n    - name: OPERATOR_EXCLUDE_NAMESPACES\n     value: \"kube-system,trivy-system\"\nEOF\n</code></pre>    The operator will be installed in the <code>trivy-system</code> namespace and will select all namespaces, except    <code>kube-system</code> and <code>trivy-system</code>. </p> </li> <li> <p>After install, watch the operator come up using the following command:    <pre><code>$ kubectl get clusterserviceversions -n trivy-system\nNAME                        DISPLAY              VERSION   REPLACES                     PHASE\ntrivy-operator.v0.4.0  Trivy Operator   0.4.0    trivy-operator.v0.3.0   Succeeded\n</code></pre>    If the above command succeeds and the ClusterServiceVersion has transitioned from <code>Installing</code> to <code>Succeeded</code> phase    you will also find the operator's Deployment in the same namespace where the Subscription is:    <pre><code>$ kubectl get deployments -n trivy-system\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           11m\n</code></pre>    If for some reason it's not ready yet, check the logs of the Deployment for errors:    <pre><code>kubectl logs deployment/trivy-operator -n trivy-system\n</code></pre></p> </li> </ol>","location":"operator/installation/olm/"},{"title":"Uninstall","text":"<p>To uninstall the operator delete the Subscription, the ClusterServiceVersion, and the OperatorGroup:</p> <pre><code>kubectl delete subscription trivy-operator-subscription -n trivy-system\nkubectl delete clusterserviceversion trivy-operator.v0.4.0 -n trivy-system\nkubectl delete operatorgroup trivy-operator-group -n trivy-system\nkubectl delete ns trivy-system\n</code></pre> <p>You have to manually delete custom resource definitions created by the OLM operator:</p>  <p>Danger</p> <p>Deleting custom resource definitions will also delete all security reports generated by the operator.</p> <pre><code>kubectl delete crd vulnerabilityreports.aquasecurity.github.io\nkubectl delete crd configauditreports.aquasecurity.github.io\nkubectl delete crd clusterconfigauditreports.aquasecurity.github.io\nkubectl delete crd exposedsecrets.aquasecurity.github.io\n</code></pre>","location":"operator/installation/olm/#uninstall"},{"title":"Upgrade","text":"<p>We recommend that you upgrade Trivy Operator often to stay up to date with the latest fixes and enhancements.</p> <p>However, at this stage we do not provide automated upgrades. Therefore, uninstall the previous version of the operator before you install the latest release.</p>  <p>Warning</p> <p>Consult release notes and changelog to revisit and migrate configuration settings which may not be compatible between different versions.</p>","location":"operator/installation/upgrade/"},{"title":"Manage Access to Security Reports","text":"<p>In Trivy-Operator security reports are stored as CRD instances (e.g. VulnerabilityReport and ConfigAuditReport objects).</p> <p>With Kubernetes RBAC, a cluster administrator can choose the following levels of granularity to manage access to security reports:</p> <ol> <li>Grant administrative access to view any report in any namespace.</li> <li>Grant coarse-grained access to view any report in a specified namespace.</li> <li>Grant fine-grained access to view a specified report in a specified namespace.</li> </ol> <p>Even though you can achieve fine-grained access control with Kubernetes RBAC configuration, it is very impractical to do so with security reports. Mainly because security reports are associated with ephemeral Kubernetes objects such as Pods and ReplicaSets.</p> <p>To sum up, we only recommend using administrative and coarse-grained levels to manage access to security reports.</p> <p>Continue reading to see examples of managing access to VulnerabilityReport objects at different levels of granularity.</p>","location":"tutorials/manage_access_to_security_reports/"},{"title":"Create Namespaces and Deployments","text":"<p>Let's consider a multitenant cluster with two <code>nginx</code> Deployments in <code>foo</code> and <code>bar</code> namespaces. There's also the <code>redis</code> Deployment in the <code>foo</code> namespace.</p> <pre><code>kubectl create namespace foo\nkubectl create deploy nginx --image nginx:1.16 --namespace foo\nkubectl create deploy redis --image redis:5 --namespace foo\n</code></pre> <pre><code>kubectl create namespace bar\nkubectl create deploy nginx --image nginx:1.16 --namespace bar\n</code></pre>  <p>Tip</p> <p>For workloads with multiple containers we'll have multiple instances of VulnerabilityReports with the same prefix (<code>replicaset-nginx-7967dc8bfd-</code>) but different suffixes that correspond to container names.</p>  <pre><code>$ kubectl tree deploy nginx --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/nginx                                           -              21m\nfoo        \u2514\u2500ReplicaSet/nginx-7967dc8bfd                              -              21m\nfoo          \u251c\u2500Pod/nginx-7967dc8bfd-gqw8h                             True           21m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-nginx-7967dc8bfd-nginx  -              4m36s\n</code></pre> <pre><code>$ kubectl tree deploy nginx --namespace bar\nNAMESPACE  NAME                                                      READY  REASON  AGE\nbar        Deployment/nginx                                          -              20m\nbar        \u2514\u2500ReplicaSet/nginx-f4cc56f6b                              -              20m\nbar          \u251c\u2500Pod/nginx-f4cc56f6b-9cd45                             True           20m\nbar          \u2514\u2500VulnerabilityReport/replicaset-nginx-f4cc56f6b-nginx  -              2m12s\n</code></pre> <pre><code>$ kubectl tree deploy redis --namespace foo\nNAMESPACE  NAME                                                       READY  REASON  AGE\nfoo        Deployment/redis                                           -              74m\nfoo        \u2514\u2500ReplicaSet/redis-79c5cc7cf8                              -              74m\nfoo          \u251c\u2500Pod/redis-79c5cc7cf8-fz99f                             True           74m\nfoo          \u2514\u2500VulnerabilityReport/replicaset-redis-79c5cc7cf8-redis  -              74m\n</code></pre>","location":"tutorials/manage_access_to_security_reports/#create-namespaces-and-deployments"},{"title":"Choose Access Level","text":"<p>To manage access to VulnerabilityReport instances a cluster administrator will typically create Role or ClusterRole objects and bind them to subjects (users, groups, or service accounts) by creating RoleBinding or ClusterRoleBinding objects.</p> <p>With Kubernetes RBAC there are three different granularity levels at which you can grant access to VulnerabilityReports:</p> <ol> <li>Cluster - a subject can view any report in any namespace</li> <li>Namespace - a subject can view any report in a specified namespace</li> <li>Security Report - a subject can view a specified report in a specified namespace</li> </ol>","location":"tutorials/manage_access_to_security_reports/#choose-access-level"},{"title":"Grant Access to View any VulnerabilityReport in any Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create clusterrolebinding dpacak-can-view-vulnerabilityreports \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as dpacak\nNAMESPACE   NAME                                REPOSITORY      TAG    SCANNER   AGE\nbar         replicaset-nginx-f4cc56f6b-nginx    library/nginx   1.16   Trivy     40m\nfoo         replicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     43m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports -A --as zpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"zpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" at the cluster scope\n</code></pre> <pre><code>$ kubectl who-can get vulnerabilityreports -A\nNo subjects found with permissions to get vulns assigned through RoleBindings\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\ndpacak-can-view-vulnerabilityreports         dpacak                     User\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>  <p>Note</p> <p>The who-can command is a kubectl plugin that shows who has RBAC permissions to perform actions on different resources in Kubernetes.</p>","location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-any-namespace"},{"title":"Grant Access to View any VulnerabilityReport in the foo Namespace","text":"<pre><code>kubectl create clusterrole view-vulnerabilityreports \\\n  --resource vulnerabilityreports \\\n  --verb get,list,watch\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-vulnerabilityreports \\\n  --namespace foo \\\n  --clusterrole view-vulnerabilityreports \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace foo --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     51m\n</code></pre> <pre><code>$ kubectl get vulnerabilityreports --namespace bar --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io is f\norbidden: User \"dpacak\" cannot list resource \"vulnerabilityreports\" in API grou\np \"aquasecurity.github.io\" in the namespace \"bar\"\n</code></pre>","location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-any-vulnerabilityreport-in-the-foo-namespace"},{"title":"Grant Access to View the replicaset-nginx-7967dc8bfd-nginx VulnerabilityReport in the foo Namespace","text":"<p>Even though you can grant access to a single VulnerabilityReport by specifying its name when you create Role or ClusterRole objects, in practice it's not manageable for these reasons:</p> <ol> <li>The name of a ReplicaSet (e.g. <code>nginx-7967dc8bfd</code>) and hence the name of the corresponding VulnerabilityReport (e.g.    <code>replicaset-nginx-7967dc8bfd-nginx</code>) change over time. This requires that Role or ClusterObject will be updated    respectively.</li> <li>We create a VulnerabilityReport for each container of a Kubernetes workload. Therefore, managing such fine-grained    permissions is even more cumbersome.</li> <li>Last but not least, the naming convention is an implementation details that's likely to change when we add support    for mutable tags or implement caching of scan results.</li> </ol> <pre><code>kubectl create role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --resource vulnerabilityreports \\\n  --resource-name replicaset-nginx-7967dc8bfd-nginx \\\n  --verb get\n</code></pre> <pre><code>kubectl create rolebinding dpacak-can-view-replicaset-nginx-7967dc8bfd-nginx \\\n  --namespace foo \\\n  --role view-replicaset-nginx-7967dc8bfd-nginx \\\n  --user dpacak\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-nginx-7967dc8bfd-nginx --as dpacak\nNAME                                REPOSITORY      TAG    SCANNER   AGE\nreplicaset-nginx-7967dc8bfd-nginx   library/nginx   1.16   Trivy     163m\n</code></pre> <pre><code>$ kubectl get vuln -n foo replicaset-redis-79c5cc7cf8-redis --as dpacak\nError from server (Forbidden): vulnerabilityreports.aquasecurity.github.io \"rep\nlicaset-redis-79c5cc7cf8-redis\" is forbidden: User \"dpacak\" cannot get resource\n\"vulnerabilityreports\" in API group \"aquasecurity.github.io\" in the namespace \"\nfoo\"\n</code></pre> <pre><code>$ kubectl who-can get vuln/replicaset-nginx-7967dc8bfd-nginx -n foo\nROLEBINDING                                        NAMESPACE  SUBJECT  TYPE  SA-NAMESPACE\ndpacak-can-view-replicaset-nginx-7967dc8bfd-nginx  foo        dpacak   User\n\nCLUSTERROLEBINDING                           SUBJECT                    TYPE            SA-NAMESPACE\ncluster-admin                                system:masters             Group\nsystem:controller:generic-garbage-collector  generic-garbage-collector  ServiceAccount  kube-system\nsystem:controller:namespace-controller       namespace-controller       ServiceAccount  kube-system\n</code></pre>","location":"tutorials/manage_access_to_security_reports/#grant-access-to-view-the-replicaset-nginx-7967dc8bfd-nginx-vulnerabilityreport-in-the-foo-namespace"},{"title":"Using the Trivy Operator addon in microk8s","text":"","location":"tutorials/microk8s/"},{"title":"Using the Trivy Operator through Microk8s","text":"<p>Microk8s is a lightweight Kubernetes distribution that can be used on your personal machine, Raspberry Pi cluster, in data centres or edge devices; just to name a few use cases.</p> <p>One of the benefits of using microk8s is its add-on ecosystem. Once you have microk8s installed, you can spin up a variety of cloud native projects directly in your cluster through merely one command:</p> <pre><code>microk8s enable &lt;name of the addon&gt;\n</code></pre> <p>A list of addons is provided below. <pre><code>    dashboard-ingress    # (community) Ingress definition for Kubernetes dashboard\n    jaeger               # (community) Kubernetes Jaeger operator with its simple config\n    knative              # (community) Knative Serverless and Event Driven Applications\n    linkerd              # (community) Linkerd is a service mesh for Kubernetes and other frameworks\n    multus               # (community) Multus CNI enables attaching multiple network interfaces to pods\n    openebs              # (community) OpenEBS is the open-source storage solution for Kubernetes\n    osm-edge             # (community) osm-edge is a lightweight SMI compatible service mesh for the edge-computing.\n    portainer            # (community) Portainer UI for your Kubernetes cluster\n    starboard            # (community) Kubernetes-native security toolkit\n    traefik              # (community) traefik Ingress controller for external access\n    dns                  # (core) CoreDNS\n    ha-cluster           # (core) Configure high availability on the current node\n    helm                 # (core) Helm - the package manager for Kubernetes\n    helm3                # (core) Helm 3 - the package manager for Kubernetes\n    trivy                # (core) Kubernetes-native security scanner\n    cert-manager         # (core) Cloud native certificate management\n    dashboard            # (core) The Kubernetes dashboard\n    host-access          # (core) Allow Pods connecting to Host services smoothly\n    hostpath-storage     # (core) Storage class; allocates storage from host directory\n    ingress              # (core) Ingress controller for external access\n    kube-ovn             # (core) An advanced network fabric for Kubernetes\n    mayastor             # (core) OpenEBS MayaStor\n    metallb              # (core) Loadbalancer for your Kubernetes cluster\n    metrics-server       # (core) K8s Metrics Server for API access to service metrics\n    observability        # (core) A lightweight observability stack for logs, traces and metrics\n    prometheus           # (core) Prometheus operator for monitoring and logging\n    rbac                 # (core) Role-Based Access Control for authorisation\n    registry             # (core) Private image registry exposed on localhost:32000\n    storage              # (core) Alias to hostpath-storage add-on, deprecated\n</code></pre></p> <p>This tutorial will showcase how to install and then remove the Trivy Operator addon.</p>","location":"tutorials/microk8s/#using-the-trivy-operator-through-microk8s"},{"title":"Prerequisits","text":"<p>You need to have microk8s installed. In our case, we have set up kubectl to use the microk8s cluster. You can find different guides, depending on your operating system, on the microk8s website.</p>","location":"tutorials/microk8s/#prerequisits"},{"title":"Install the Trivy Operator","text":"<p>To install the Trivy Operator, simply run the following command: <pre><code>microk8s enable trivy\n</code></pre></p> <p>The confirmation should be similar to the following output: <pre><code>Infer repository core for addon trivy\nInfer repository core for addon helm3\nAddon core/helm3 is already enabled\nInfer repository core for addon dns\nAddon core/dns is already enabled\nInstalling Trivy\n\"aqua\" already exists with the same configuration, skipping\nRelease \"trivy-operator\" does not exist. Installing it now.\nNAME: trivy-operator\nLAST DEPLOYED: Sat Oct  8 16:39:59 2022\nNAMESPACE: trivy-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nYou have installed Trivy Operator in the trivy-system namespace.\nIt is configured to discover Kubernetes workloads and resources in\nall namespace(s).\n\nInspect created VulnerabilityReports by:\n\n    kubectl get vulnerabilityreports --all-namespaces -o wide\n\nInspect created ConfigAuditReports by:\n\n    kubectl get configauditreports --all-namespaces -o wide\n\nInspect the work log of trivy-operator by:\n\n    kubectl logs -n trivy-system deployment/trivy-operator\nTrivy is installed\n</code></pre></p> <p>You should now see the Trivy Operator pod running inside of the <code>trivy-system</code> namespace: <pre><code>kubectl get all -n trivy-system\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/trivy-operator-57c44575c4-ml2hw             1/1     Running   0          29s\npod/scan-vulnerabilityreport-5d55f55cd7-7l6kn   1/1     Running   0          27s\n\nNAME                     TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nservice/trivy-operator   ClusterIP   None         &lt;none&gt;        80/TCP    29s\n\nNAME                             READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/trivy-operator   1/1     1            1           29s\n\nNAME                                        DESIRED   CURRENT   READY   AGE\nreplicaset.apps/trivy-operator-57c44575c4   1         1         1       29s\n\nNAME                                            COMPLETIONS   DURATION   AGE\njob.batch/scan-vulnerabilityreport-5d55f55cd7   0/1           27s        27s\n</code></pre></p> <p>If you have any container images running in your microk8s cluster, Trivy will start a vulnerability scan on those right away. </p>","location":"tutorials/microk8s/#install-the-trivy-operator"},{"title":"Cleaning up","text":"<p>Remvoing the Trivy Operator from your cluster is as easy as installing it. Simply run: <pre><code>microk8s disable trivy\n</code></pre></p> <p>You should see an output similar to the following: <pre><code>Infer repository core for addon trivy\nDisabling Trivy\nrelease \"trivy-operator\" uninstalled\nTrivy disabled\n</code></pre></p>","location":"tutorials/microk8s/#cleaning-up"},{"title":"Allow the Trivy Operator to access private registries","text":"<p>In this tutorial, we will detail multiple ways on setting up the Trivy Operator to access and scan container images from private container registries.</p>","location":"tutorials/private-registries/"},{"title":"Prerequisites","text":"<p>To follow this tutorial, you need the following installed on your machine:</p> <ul> <li>The Helm CLI tool</li> <li>kubectl and connected to a running Kubernetes cluster</li> <li>a container images in a private registy, running as a pod inside your cluster</li> </ul> <p>Note that we will be using a local Kubernetes KinD cluster and a private container image stored on a private GitHub repository for the examples.</p>","location":"tutorials/private-registries/#prerequisites"},{"title":"First Option: Filesystem Scanning","text":"<p>For this tutorial, we will use the Operator Helm Chart.</p> <p>The configuration options for the Helm Chart can be found in the values.yaml manifest. Navigate to the section <code>Trivy.command</code>. The default will be:</p> <pre><code>trivy:\n# command. Either `image` or `filesystem` scanning, depending on the target type required for the scan.\n  # For 'filesystem' scanning, ensure that the `trivyOperator.scanJobPodTemplateContainerSecurityContext` is configured\n  # to run as the root user (runAsUser = 0).\n  command: image\n</code></pre> <p>By default, the command that trivy is supposed to run inside your cluster is <code>trivy image</code> for container image scanning. However, we want to change it to scan the filesystem in your nodes instead. Container images are ultimately stored as files on the node level of your cluster. This way, trivy is going to scan the files of your container images for vulnerabilities. This is a little bit of a work-around with the downside that the Trivy Operator will have to run as root. However, remember that security scanning already requires the operator to have lots of cluster privileges.</p> <p>Next, we will change the the <code>command</code> and the <code>trivyOperator.scanJobPodTemplateContainerSecurityContext</code>of the <code>values.yaml</code> manifest. For this, we can create a new values.yaml manifest with our desired modifiactions: <pre><code>trivy:\n    command: fs\n    ignoreUnfixed: true\ntrivyOperator:\n    scanJobPodTemplateContainerSecurityContext:\n        # For filesystem scanning, Trivy needs to run as the root user\n        runAsUser: 0\n</code></pre></p> <p>Lastly, we can deploy the operator inside our cluster with referencing our new <code>values.yaml</code> manifest to override the default values:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.4.0\n  --values ./values.yaml\n</code></pre> <p>Alternatively, it is possible to set the values directly through Helm instead of referencing an additional <code>values.yaml</code> file: <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.4.0\n  --set=\"trivy.command=fs\"\n  --set=\"trivyOperator.scanJobPodTemplateContainerSecurityContext.runAsUser=0\"\n</code></pre></p> <p>Once installed, make sure that </p> <ol> <li>the operator is running in your cluster</li> <li>the operator has created a VulnerabilitReport for the container image from the private registry</li> </ol> <pre><code>\u276f kubectl get deployment -n trivy-system\n\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           99s\n</code></pre>","location":"tutorials/private-registries/#first-option-filesystem-scanning"},{"title":"Second Option: Using an ImagePullSecret to access containers from the Private Registry","text":"<p>Note that you might be using an ImagePullSecret already to allow pods to pull the container images from a private registry. </p> <p>To set-up an ImagePullSecret, we first need an access token to our private registry. For GitHub private registries, you can create a new access token under the following link. In comparison, the official Kubernetes coumentation shows how to create the ImagePullSecret for the DockerHub.</p> <p>Next, we will base64 encode the access token:</p> <pre><code>echo -n \"YOUR_GH_ACCOUNT_NAME:YOUR_TOKEN\" | base64\n</code></pre> <p>And take the output of the previous command and parse it into the following to base64 encode it again: <pre><code>echo -n  '{\"auths\":{\"ghcr.io\":{\"auth\":\"OUTPUT\"}}}' | base64\n</code></pre></p> <p>Lastly, we are going to store the output in a Kubernetes Secret YAML manifest:</p> <p><code>imagepullsecret.yaml</code> <pre><code>kind: Secret\ntype: kubernetes.io/dockerconfigjson\napiVersion: v1\nmetadata:\n  name: dockerconfigjson-github-com\n  labels:\n    app: app-name\ndata:\n  .dockerconfigjson: OUTPUT\n</code></pre></p> <p>Note that base64 encoding is not encryption, thust, you should not commit this file. If you are looking to store secrets in Kubernetes, have a look at e.g. Hashicorp Secret Vault, or with an External Secrets Operator (ESO).</p> <p>Make sure to reference the ImagePullSecret in your container <code>spec</code>: <pre><code>containers:\n- name: cns-website\n  image: ghcr.io/account-name/image-id:tag\nimagePullSecrets:\n- name: dockerconfigjson-github-com\n</code></pre></p> <p>And finally, we can apply the secret to the same namespace as our application: <pre><code>kubectl apply -f imagepullsecret.yaml -n app\n</code></pre></p> <p>If you have to modify your deployment.yaml manifest, make sure to update that as well.</p> <p>Once you have defined your ImagePullSecret, the Operator will have access to the container image automatically with the defaul configuration.</p>","location":"tutorials/private-registries/#second-option-using-an-imagepullsecret-to-access-containers-from-the-private-registry"},{"title":"Third Option: Define an ImagePullSecret through a ServiceAccount","text":"<p>Alternatively to defining an ImagePullSecret on the pod level, we can also define the secret through a Kubernetes Service Account. Our workload will be associated with the service account and can pull the secret from our private registry.  Similar to the <code>Second Option</code>, once we have the key associated to our workload, the Trivy operator scan job has access to the secret and can pull the image.</p> <p>Again, you can have a look at the official Kubernetes documentation for further details.</p> <p><code>imagepullsecret.yaml</code> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: dockerconfigjson-sa-github-com\n  annotations:\n    kubernetes.io/service-account.name: cns-website\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: OUTPUT\n</code></pre></p> <p><code>serviceaccount.yaml</code> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cns-website\nimagePullSecrets:\n- name: dockerconfigjson-sa-github-com\n</code></pre></p> <p><code>deployment.yaml</code> *or where you have defined your container <pre><code>spec:\n    containers:\n    - name: cns-website\n      image: ghcr.io/account-name/image-id:tag\n    serviceAccountName: cns-website\n</code></pre></p>","location":"tutorials/private-registries/#third-option-define-an-imagepullsecret-through-a-serviceaccount"},{"title":"Fourth Option: Define Secrets through Trivy-Operator configuration","text":"<p>If there are no ImagePullSecret on pod or Service Account level (for example, valid credentials are placed in container runtime configuration) you can add them in Trivy-Operator configuration.</p> <p>It's very similar to <code>Second Option</code>. First of all you need to create a secret. To do it, we first need an access token to our private registry. For GitHub private registries, you can create a new access token under the following link. In comparison, the official Kubernetes coumentation shows how to create the ImagePullSecret for the DockerHub.</p> <p>Next, we will base64 encode the access token:</p> <pre><code>echo -n \"YOUR_GH_ACCOUNT_NAME:YOUR_TOKEN\" | base64\n</code></pre> <p>And take the output of the previous command and parse it into the following to base64 encode it again: <pre><code>echo -n  '{\"auths\":{\"ghcr.io\":{\"auth\":\"OUTPUT\"}}}' | base64\n</code></pre></p> <p>Lastly, we are going to store the output in a Kubernetes Secret YAML manifest:</p> <p><code>imagepullsecret.yaml</code> <pre><code>kind: Secret\ntype: kubernetes.io/dockerconfigjson\napiVersion: v1\nmetadata:\n  name: dockerconfigjson-github-com\n  labels:\n    app: app-name\ndata:\n  .dockerconfigjson: OUTPUT\n</code></pre></p> <p>Note that base64 encoding is not encryption, thust, you should not commit this file. If you are looking to store secrets in Kubernetes, have a look at e.g. Hashicorp Secret Vault, or with an External Secrets Operator (ESO).</p> <p>And finally, we can apply the secret to the same namespace as our application: <pre><code>kubectl apply -f imagepullsecret.yaml -n app\n</code></pre></p> <p>Next, we will change the <code>privateRegistryScanSecretsNames</code> of the <code>values.yaml</code> manifest. For this, we can create a new <code>values.yaml</code> manifest with our desired modifiactions. We need to provide desired namespace and secret name. In our example they are <code>app</code> and <code>dockerconfigjson-github-com</code> accordingly.</p> <pre><code>operator:\n    privateRegistryScanSecretsNames: {\"app\":\"dockerconfigjson-github-com\"}\n</code></pre> <p>If you want you can add additional namespaces and secret names to <code>privateRegistryScanSecretsNames</code> separated by comma.</p> <p>Lastly, we can deploy the operator inside our cluster with referencing our new <code>values.yaml</code> manifest to override the default values:</p> <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.4.0\n  --values ./values.yaml\n</code></pre> <p>Alternatively, it is possible to set the values directly through Helm instead of referencing an additional <code>values.yaml</code> file: <pre><code>helm upgrade --install trivy-operator aqua/trivy-operator \\\n  --namespace trivy-system \\\n  --create-namespace \\\n  --version 0.4.0\n  --set-json='operator.privateRegistryScanSecretsNames={\"app\":\"dockerconfigjson-github-com\"}'\n</code></pre></p> <p>Works only with helm 3.10+, because <code>--set-json</code> flag was added in 3.10.0. Otherwise you can use <code>values.yaml</code> instead.</p> <p>Once installed, make sure that </p> <ol> <li>the operator is running in your cluster</li> <li>the operator has created a VulnerabilitReport for the container image from the private registry</li> </ol> <pre><code>\u276f kubectl get deployment -n trivy-system\n\nNAME             READY   UP-TO-DATE   AVAILABLE   AGE\ntrivy-operator   1/1     1            1           99s\n</code></pre>","location":"tutorials/private-registries/#fourth-option-define-secrets-through-trivy-operator-configuration"},{"title":"Fifth Option: Grant access through managed registries","text":"<p>The last way that you could give the Trivy operator access to your private container registry is through managed registries. In this case, the container registry and your Kubernetes cluster would have to be on the same cloud provider; then you can define access to your container namespace as part of the IAM account. Once defined, trivy will already have the permissions for the registry.</p> <p>For additional information, please refer to the documentation on managed registries.</p>","location":"tutorials/private-registries/#fifth-option-grant-access-through-managed-registries"},{"title":"Writing Custom Configuration Audit Policies","text":"<p>trivy-operator ships with a set of Built-in Configuration Audit Policies defined as OPA Rego policies. You can also define custom policies and associate them with applicable Kubernetes resources to extend basic configuration audit functionality.</p> <p>This tutorial will walk through the process of creating and testing a new configuration audit policy that fails whenever a Kubernetes resource doesn't specify <code>app.kubernetes.io/name</code> or <code>app.kubernetes.io/version</code> labels.</p>","location":"tutorials/writing-custom-configuration-audit-policies/"},{"title":"Writing a Policy","text":"<p>To define such a policy, you must first define its metadata. This includes setting a unique identifier, title, severity (<code>CRITICAL</code>, <code>HIGH</code>, <code>MEDIUM</code>, <code>LOW</code>), descriptive text, and remediation steps. In Rego it's defined as the <code>__rego_metadata__</code> rule, which defines the following composite value:</p> <pre><code>package trivyoperator.policy.k8s.custom\n\nimport data.lib.result\nimport future.keywords.in\n\n__rego_metadata__ := {\n    \"id\": \"recommended_labels\",\n    \"title\": \"Recommended labels\",\n    \"severity\": \"LOW\",\n    \"type\": \"Kubernetes Security Check\",\n    \"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.\",\n    \"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n    \"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n}\n</code></pre> <p>Note that the <code>recommended_labels</code> policy in scoped to the <code>trivyoperator.policy.k8s.custom</code> package to avoid naming collision with built-in policies that are pre-installed with trivy-operator.</p> <p>Once we've got our metadata defined, we need to create the logic of the policy, which is done in the <code>deny</code> or <code>warn</code> rule.</p> <pre><code>__rego_input__ := {\n    \"combine\": false,\n    \"selector\": [{\"type\": \"kubernetes\"}],\n}\n\ndeny[res] {\n    input.kind == \"Pod\"\n    some container in input.spec.containers\n    not startswith(container.image, \"hooli.com\")\n    msg := sprintf(\"Image '%v' comes from untrusted registry\", [container.image])\n    res := result.new(msg, container)\n}\n</code></pre> <p>These matches are essentially Rego assertions, so anyone familiar with writing rules for OPA or other tools that use Rego should find the process familiar. In this case, it\u2019s pretty straightforward. We subtract the set of labels specified by the <code>input</code> resource object from the set of recommended labels. The resulting set is stored in the variable called <code>missing</code>. Finally, we check if the <code>missing</code> set is empty. If not, the <code>deny</code> rule fails with the appropriate message.</p> <p>The <code>input</code> document is set by trivy-operator to a Kubernetes resource when the policy is evaluated. For pods, it would look something like the following listing:</p> <pre><code>{\n  \"apiVersion\": \"v1\",\n  \"kind\": \"Pod\",\n  \"metadata\": {\n    \"name\": \"nginx\",\n    \"labels\": {\n      \"run\": \"nginx\"\n    }\n  },\n  \"spec\": {\n    \"containers\": [\n      {\n        \"name\": \"nginx\",\n        \"image\": \"nginx:1.16\",\n      }\n    ]\n  }\n}\n</code></pre> <p>The labels set on the pod resource above can be retrieved with the following Rego expression:</p> <pre><code>provided := {label | input.metadata.labels[label]}\n</code></pre> <p>You can find the complete Rego code listing in recommended_labels.rego.</p>","location":"tutorials/writing-custom-configuration-audit-policies/#writing-a-policy"},{"title":"Testing a Policy","text":"<p>Now that you've created the policy, you need to test it to make sure it works as intended. To do that, add policy code to the <code>trivy-operator-policies-config</code> ConfigMap and associate it with any (<code>*</code>) Kubernetes resource kind:</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: trivy-operator-policies-config\n  namespace: trivy-system\n  labels:\n    app.kubernetes.io/name: trivy-operator\n    app.kubernetes.io/instance: trivy-operator\n    app.kubernetes.io/version: \"0.4.0\"\n    app.kubernetes.io/managed-by: kubectl\ndata:\n  policy.recommended_labels.kinds: \"*\"\n  policy.recommended_labels.rego: |\n  package trivyoperator.policy.k8s.custom\n\n   import data.lib.result\n   import future.keywords.in\n\n   __rego_metadata__ := {\n      \"id\": \"recommended_labels\",\n      \"title\": \"Recommended labels\",\n      \"severity\": \"LOW\",\n      \"type\": \"Kubernetes Security Check\",\n      \"description\": \"A common set of labels allows tools to work interoperably, describing objects in a common manner that all tools can understand.\",\n      \"recommended_actions\": \"Take full advantage of using recommended labels and apply them on every resource object.\",\n      \"url\": \"https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/\",\n  }\n\n   __rego_input__ := {\n      \"combine\": false,\n      \"selector\": [{\"type\": \"kubernetes\"}],\n  }\n\n   deny[res] {\n      input.kind == \"Pod\"\n      some container in input.spec.containers\n      not startswith(container.image, \"hooli.com\")\n      msg := sprintf(\"Image '%v' comes from untrusted registry\", [container.image])\n      res := result.new(msg, container)\n  }\n</code></pre> <p>In this example, to add a new policy, you must define two data entries in the <code>trivy-operator-policies-config</code> ConfigMap:</p> <ol> <li>The <code>policy.&lt;your_policy_name&gt;.kinds</code> entry is used to designate applicable Kubernetes resources as a comma separated    list of Kubernetes kinds (e.g., <code>Pod,ConfigMap,NetworkPolicy</code>). There is also a special value (<code>Workload</code>) that you    can use to select all Kubernetes workloads, and (<code>*</code>) to select all Kubernetes resources recognized by trivy-operator.</li> <li>The <code>policy.&lt;your_policy_name&gt;.rego</code> entry holds the policy Rego code.</li> </ol> <p>trivy-operator automatically detects policies added to the <code>trivy-operator-policies-config</code> ConfigMap and immediately rescans applicable Kubernetes resources.</p> <p>Let's create the <code>test</code> ConfigMap without recommended labels:</p> <pre><code>$ kubectl create cm test --from-literal=foo=bar\nconfigmap/test created\n</code></pre> <p>When you retrieve the corresponding configuration audit report, you'll see that there is one check with <code>LOW</code> severity that's failing:</p> <pre><code>$ kubectl get configauditreport configmap-test -o wide\nNAME             SCANNER     AGE   CRITICAL  HIGH   MEDIUM   LOW\nconfigmap-test   trivy-operator   24s   0         0      0        1\n</code></pre> <p>If you describe the report you'll see that it's failing because of our custom policy:</p> <pre><code>apiVersion: aquasecurity.github.io/v1alpha1\nkind: ConfigAuditReport\nmetadata:\n  labels:\n    trivy-operator.resource.kind: ConfigMap\n    trivy-operator.resource.name: test\n    trivy-operator.resource.namespace: default\n    plugin-config-hash: df767ff5f\n    resource-spec-hash: 7c96769cf\n  name: configmap-test\n  namespace: default\n  ownerReferences:\n  - apiVersion: v1\n    blockOwnerDeletion: false\n    controller: true\n    kind: ConfigMap\n    name: test\nreport:\n  scanner:\n    name: trivy-operator\n    vendor: Aqua Security\n    version: v0.4.0\n  summary:\n    criticalCount: 0\n    highCount: 0\n    lowCount: 1\n    mediumCount: 0\n  checks:\n  - checkID: recommended_labels  # (1)\n    title: Recommended labels    # (2)\n    severity: LOW                # (3)\n    category: Kubernetes Security Check  # (4)\n    description: |                       # (5)\n      A common set of labels allows tools to work interoperably,\n      describing objects in a common manner that all tools can\n      understand.\n    success: false  # (6)\n    messages:       # (7)\n    - 'You must provide labels: {\"app.kubernetes.io/name\", \"app.kubernetes.io/version\"}'\n</code></pre> <ol> <li>The <code>checkID</code> property corresponds to the policy identifier, i.e. <code>__rego_meatadata__.id</code>.</li> <li>The <code>title</code> property as defined by the policy metadata in <code>__rego_metadata__.title</code>.</li> <li>The <code>severity</code> property as defined by the policy metadata in <code>__rego_metadata__.severity</code>.</li> <li>The <code>category</code> property as defined by the policy metadata in <code>__rego_metadata__.type</code>.</li> <li>The <code>description</code> property as defined by the policy metadata in <code>__rego_metadata__.description</code>.</li> <li>The flag indicating whether the configuration audit check has failed or passed.</li> <li>The array of messages with details in case of failure.</li> </ol>","location":"tutorials/writing-custom-configuration-audit-policies/#testing-a-policy"},{"title":"Vulnerability Scanners","text":"<p>Vulnerability scanning is an important way to identify and remediate security gaps in Kubernetes workloads. The process involves scanning container images to check all software on them and report any vulnerabilities found.</p> <p>Trivy Operator automatically discovers and scans all images that are being used in a Kubernetes cluster, including images of application pods and system pods. Scan reports are saved as [VulnerabilityReport] resources, which are owned by a Kubernetes controller.</p> <p>For example, when Trivy scans a Deployment, the corresponding VulnerabilityReport instance is attached to its current revision. In other words, the VulnerabilityReport inherits the life cycle of the Kubernetes controller. This also implies that when a Deployment is rolling updated, it will get scanned automatically, and a new instance of the VulnerabilityReport will be created and attached to the new revision. On the other hand, if the previous revision is deleted, the corresponding VulnerabilityReport will be deleted automatically by the Kubernetes garbage collector.</p> <p>Trivy may scan Kubernetes workloads that run images from Private Registries and certain Managed Registries.</p>","location":"vulnerability-scanning/"},{"title":"Managed Registries","text":"","location":"vulnerability-scanning/managed-registries/"},{"title":"Amazon Elastic Container Registry (ECR)","text":"<p>You must create an IAM OIDC identity provider for your cluster:</p> <pre><code>eksctl utils associate-iam-oidc-provider \\\n  --cluster &lt;cluster_name&gt; \\\n  --approve\n</code></pre> <p>Override the existing <code>trivy-operator</code> service account and attach the IAM policy to grant it permission to pull images from the ECR:</p> <pre><code>eksctl create iamserviceaccount \\\n  --name trivy-operator \\\n  --namespace trivy-system \\\n  --cluster &lt;cluster_name&gt; \\\n  --attach-policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly \\\n  --approve \\\n  --override-existing-serviceaccounts\n</code></pre>","location":"vulnerability-scanning/managed-registries/#amazon-elastic-container-registry-ecr"},{"title":"Azure Container Registry (ACR)","text":"<p>Before you can start, you need to install <code>aad-pod-identity</code> inside your cluster, see installation instructions: https://azure.github.io/aad-pod-identity/docs/getting-started/installation/</p> <p>Create a managed identity and assign the permission to the ACR. <pre><code>export IDENTITY_NAME=trivy-operator-identity\nexport AZURE_RESOURCE_GROUP=&lt;my_resource_group&gt;\nexport AZURE_LOCATION=westeurope\nexport ACR_NAME=&lt;my_azure_container_registry&gt;\n\naz identity create --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --location ${AZURE_LOCATION}\n\nexport IDENTITY_ID=(az identity show --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --query id -o tsv)\nexport IDENTITY_CLIENT_ID=$(az identity show --name ${IDENTITY_NAME} --resource-group ${AZURE_RESOURCE_GROUP} --query clientId -o tsv)\nexport ACR_ID=$(az acr show --name ${ACR_NAME} --query id -o tsv)\n\naz role assignment create --assignee ${IDENTITY_CLIENT_ID} --role 'AcrPull' --scope ${ACR_ID}\n</code></pre></p> <p>create an <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> resource inside your kubernetes cluster: <pre><code>apiVersion: aadpodidentity.k8s.io/v1\nkind: AzureIdentity\nmetadata:\n  name: trivy-identity\n  namespace: trivy-system\nspec:\n  clientID: ${IDENTITY_ID}\n  resourceID: ${IDENTITY_CLIENT_ID}\n  type: 0\n</code></pre></p> <pre><code> apiVersion: aadpodidentity.k8s.io/v1\n kind: AzureIdentityBinding\n metadata:\n   name: trivy-id-binding\n   namespace: trivy-system\n spec:\n   azureIdentity: trivy-operator-identity\n   selector: trivy-operator-label\n</code></pre> <p>add <code>scanJob.podTemplateLabels</code> to the Trivy Operator config map, the value must match the <code>AzureIdentityBinding</code> selector.</p> <pre><code>kubectl -n trivy-system edit cm trivy-operator\n# Insert scanJob.podTemplateLabels: aadpodidbinding=trivy-operator-label in data block\n\n# validate\nkubectl -ntrivy-system get cm trivy-operator -o jsonpath='{.data.scanJob\\.podTemplateLabels}'\n</code></pre>","location":"vulnerability-scanning/managed-registries/#azure-container-registry-acr"},{"title":"Private Registries","text":"","location":"vulnerability-scanning/private-registries/"},{"title":"Image Pull Secrets","text":"<p></p> <ol> <li>Find references to image pull secrets (direct references and via service account).</li> <li>Create the temporary secret with basic credentials for each container of the scanned workload.</li> <li>Create the scan job that references the temporary secret. The secret has the ownerReference property set to point to the job.</li> <li>Watch the job until it's completed or failed.</li> <li>Parse logs and save vulnerability reports in etcd.</li> <li>Delete the job. The temporary secret will be deleted by the Kubernetes garbage collector.</li> </ol>","location":"vulnerability-scanning/private-registries/#image-pull-secrets"},{"title":"Vulnerability Scanning Configuration","text":"","location":"vulnerability-scanning/trivy/"},{"title":"Standalone","text":"<p>The default configuration settings enable Trivy <code>vulnerabilityReports.scanner</code> in <code>Standalone</code> <code>trivy.mode</code>. Even though it doesn't require any additional setup, it's the least efficient method. Each Pod created by a scan Job has the init container that downloads the Trivy vulnerabilities database from the GitHub releases page and stores it in the local file system of the emptyDir volume. This volume is then shared with containers that perform the actual scanning. Finally, the Pod is deleted along with the emptyDir volume.</p> <p></p> <p>The number of containers defined by a scan Job equals the number of containers defined by the scanned Kubernetes workload, so the cache in this mode is useful only if the workload defines multiple containers.</p> <p>Beyond that, frequent downloads from GitHub might lead to a rate limiting problem. The limits are imposed by GitHub on all anonymous requests originating from a given IP. To mitigate such problems you can add the <code>trivy.githubToken</code> key to the <code>trivy-operator</code> secret.</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.githubToken\": \"$(echo -n &lt;GITHUB_TOKEN&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre>","location":"vulnerability-scanning/trivy/#standalone"},{"title":"ClientServer","text":"<p>You can connect Trivy to an external Trivy server by changing the default <code>trivy.mode</code> from <code>Standalone</code> to <code>ClientServer</code> and specifying <code>trivy.serverURL</code>.</p> <pre><code>kubectl patch cm trivy-operator-trivy-config -n trivy-system \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.mode\":      \"ClientServer\",\n    \"trivy.serverURL\": \"&lt;TRIVY_SERVER_URL&gt;\"\n  }\n}\nEOF\n)\"\n</code></pre> <p>The Trivy server could be your own deployment, or it could be an external service. See Trivy server documentation for more information.</p> <p>If the server requires access token and/or custom HTTP authentication headers, you may add <code>trivy.serverToken</code> and <code>trivy.serverCustomHeaders</code> properties to the Trivy Operator secret.</p> <pre><code>kubectl patch secret trivy-operator-trivy-config -n trivy-system \\\n  --type merge \\\n  -p \"$(cat &lt;&lt;EOF\n{\n  \"data\": {\n    \"trivy.serverToken\":         \"$(echo -n &lt;SERVER_TOKEN&gt; | base64)\",\n    \"trivy.serverCustomHeaders\": \"$(echo -n x-api-token:&lt;X_API_TOKEN&gt; | base64)\"\n  }\n}\nEOF\n)\"\n</code></pre> <p></p>","location":"vulnerability-scanning/trivy/#clientserver"},{"title":"Settings","text":"CONFIGMAP KEY DEFAULT DESCRIPTION     <code>trivy.repository</code> <code>ghcr.io/aquasecurity/trivy</code> Repository of the Trivy image   <code>trivy.tag</code> <code>0.31.3</code> Version of the Trivy image   <code>trivy.dbRepository</code> <code>ghcr.io/aquasecurity/trivy-db</code> External OCI Registry to download the vulnerability database   <code>trivy.dbRepositoryInsecure</code> <code>false</code> The Flag to enable insecure connection for downloading trivy-db via proxy (air-gaped env)   <code>trivy.mode</code> <code>Standalone</code> Trivy client mode. Either <code>Standalone</code> or <code>ClientServer</code>. Depending on the active mode other settings might be applicable or required.   <code>additionalVulnerabilityReportFields</code> N/A A comma separated list of additional fields which can be added to the VulnerabilityReport. Possible values: <code>Description,Links,CVSS,Target</code>. Description will add more data about vulnerability. Links - all the references to a specific vulnerability. CVSS - data about CVSSv2/CVSSv3 scoring and vectors. Target - vulnerable element.   <code>trivy.command</code> <code>image</code> command. Either <code>image</code> or <code>filesystem</code> scanning. Depending on the target type required for the scan.   <code>trivy.severity</code> <code>UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL</code> A comma separated list of severity levels reported by Trivy   <code>trivy.ignoreUnfixed</code> N/A Whether to show only fixed vulnerabilities in vulnerabilities reported by Trivy. Set to <code>\"true\"</code> to enable it.   <code>trivy.skipFiles</code> N/A A comma separated list of file paths for Trivy to skip traversal.   <code>trivy.skipDirs</code> N/A A comma separated list of directories for Trivy to skip traversal.   <code>trivy.ignoreFile</code> N/A It specifies the <code>.trivyignore</code> file which contains a list of vulnerability IDs to be ignored from vulnerabilities reported by Trivy.   <code>trivy.timeout</code> <code>5m0s</code> The duration to wait for scan completion   <code>trivy.serverURL</code> N/A The endpoint URL of the Trivy server. Required in <code>ClientServer</code> mode.   <code>trivy.serverTokenHeader</code> <code>Trivy-Token</code> The name of the HTTP header to send the authentication token to Trivy server. Only application in <code>ClientServer</code> mode when <code>trivy.serverToken</code> is specified.   <code>trivy.serverInsecure</code> N/A The Flag to enable insecure connection to the Trivy server.   <code>trivy.insecureRegistry.&lt;id&gt;</code> N/A The registry to which insecure connections are allowed. There can be multiple registries with different registry <code>&lt;id&gt;</code>.   <code>trivy.nonSslRegistry.&lt;id&gt;</code> N/A A registry without SSL. There can be multiple registries with different registry <code>&lt;id&gt;</code>.   <code>trivy.registry.mirror.&lt;registry&gt;</code> N/A Mirror for the registry <code>&lt;registry&gt;</code>, e.g. <code>trivy.registry.mirror.index.docker.io: mirror.io</code> would use <code>mirror.io</code> to get images originated from <code>index.docker.io</code>   <code>trivy.httpProxy</code> N/A The HTTP proxy used by Trivy to download the vulnerabilities database from GitHub.   <code>trivy.httpsProxy</code> N/A The HTTPS proxy used by Trivy to download the vulnerabilities database from GitHub.   <code>trivy.noProxy</code> N/A A comma separated list of IPs and domain names that are not subject to proxy settings.   <code>trivy.resources.requests.cpu</code> <code>100m</code> The minimum amount of CPU required to run Trivy scanner pod.   <code>trivy.resources.requests.memory</code> <code>100M</code> The minimum amount of memory required to run Trivy scanner pod.   <code>trivy.resources.limits.cpu</code> <code>500m</code> The maximum amount of CPU allowed to run Trivy scanner pod.   <code>trivy.resources.limits.memory</code> <code>500M</code> The maximum amount of memory allowed to run Trivy scanner pod.       SECRET KEY DESCRIPTION     <code>trivy.githubToken</code> The GitHub access token used by Trivy to download the vulnerabilities database from GitHub. Only applicable in <code>Standalone</code> mode.   <code>trivy.serverToken</code> The token to authenticate Trivy client with Trivy server. Only applicable in <code>ClientServer</code> mode.   <code>trivy.serverCustomHeaders</code> A comma separated list of custom HTTP headers sent by Trivy client to Trivy server. Only applicable in <code>ClientServer</code> mode.","location":"vulnerability-scanning/trivy/#settings"}]}